[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website everytime before you want to upload changes"
  },
  {
    "objectID": "content/01_journal/02_supervised_ml.html",
    "href": "content/01_journal/02_supervised_ml.html",
    "title": "02 Supervised ML",
    "section": "",
    "text": "library(tidymodels)\nlibrary(broom.mixed)\nlibrary(xgboost)\nlibrary(tidyverse)\nlibrary(parsnip)\nlibrary(recipes)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(rpart.plot)\nlibrary(modeldata)\n# Data exploration\nbike_data_tbl &lt;- readRDS(\"C:/Users/risho/OneDrive/Desktop/internship_sparks/ss24-bdml-rishon1234/ML/data/Business Decisions with Machine Learning/bike_orderlines.rds\")\nmodel_sales_tbl &lt;- bike_data_tbl %&gt;%\n  select(total_price, model, category_2, frame_material) %&gt;%\n  \n  group_by(model, category_2, frame_material) %&gt;%\n  summarise(total_sales = sum(total_price)) %&gt;%\n  ungroup() %&gt;%\n  \n  arrange(desc(total_sales))\n\n#&gt; `summarise()` has grouped output by 'model', 'category_2'. You can override\n#&gt; using the `.groups` argument.\n\nmodel_sales_tbl %&gt;%\n  mutate(category_2 = as_factor(category_2) %&gt;% \n           fct_reorder(total_sales, .fun = max) %&gt;% \n           fct_rev()) %&gt;%\n  \n  ggplot(aes(frame_material, total_sales)) +\n  geom_violin() +\n  geom_jitter(width = 0.1, alpha = 0.5, color = \"#800080\") +\n  #coord_flip() +\n  facet_wrap(~ category_2) +\n  scale_y_continuous(labels = scales::dollar_format(scale = 1e-6, suffix = \"M\", accuracy = 0.1)) +\n  tidyquant::theme_tq() +\n  labs(\n    title = \"Total Sales for Each Model\",\n    x = \"Frame Material\", y = \"Revenue\"\n  )\n\n#&gt; Registered S3 method overwritten by 'quantmod':\n#&gt;   method            from\n#&gt;   as.zoo.data.frame zoo\n\n\n#&gt; Warning: Groups with fewer than two datapoints have been dropped.\n#&gt; ℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n#&gt; Groups with fewer than two datapoints have been dropped.\n#&gt; ℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n#&gt; Groups with fewer than two datapoints have been dropped.\n#&gt; ℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\n#&gt; Warning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\n#&gt; returning -Inf\n\n\n#&gt; Warning: Computation failed in `stat_ydensity()`.\n#&gt; Caused by error in `$&lt;-.data.frame`:\n#&gt; ! replacement has 1 row, data has 0\n\n\n\n\n\n\n\n\nbike_features_tbl &lt;- readRDS(\"C:/Users/risho/OneDrive/Desktop/internship_sparks/ss24-bdml-rishon1234/ML/data/Business Decisions with Machine Learning/bike_features_tbl.rds\")\nbike_features_tbl &lt;- bike_features_tbl %&gt;% \n  select(frame_material:gender, `Rear Derailleur`, `Shift Lever`) %&gt;% \n  mutate(\n    `shimano dura-ace`        = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano dura-ace \") %&gt;% as.numeric(),\n    `shimano ultegra`         = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano ultegra \") %&gt;% as.numeric(),\n    `shimano 105`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano 105 \") %&gt;% as.numeric(),\n    `shimano tiagra`          = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano tiagra \") %&gt;% as.numeric(),\n    `Shimano sora`            = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano sora\") %&gt;% as.numeric(),\n    `shimano deore`           = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano deore(?! xt)\") %&gt;% as.numeric(),\n    `shimano slx`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano slx\") %&gt;% as.numeric(),\n    `shimano grx`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano grx\") %&gt;% as.numeric(),\n    `Shimano xt`              = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano deore xt |shimano xt \") %&gt;% as.numeric(),\n    `Shimano xtr`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano xtr\") %&gt;% as.numeric(),\n    `Shimano saint`           = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano saint\") %&gt;% as.numeric(),\n    `SRAM red`                = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram red\") %&gt;% as.numeric(),\n    `SRAM force`              = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram force\") %&gt;% as.numeric(),\n    `SRAM rival`              = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram rival\") %&gt;% as.numeric(),\n    `SRAM apex`               = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram apex\") %&gt;% as.numeric(),\n    `SRAM xx1`                = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram xx1\") %&gt;% as.numeric(),\n    `SRAM x01`                = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram x01|sram xo1\") %&gt;% as.numeric(),\n    `SRAM gx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram gx\") %&gt;% as.numeric(),\n    `SRAM nx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram nx\") %&gt;% as.numeric(),\n    `SRAM sx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram sx\") %&gt;% as.numeric(),\n    `SRAM sx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram sx\") %&gt;% as.numeric(),\n    `Campagnolo potenza`      = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"campagnolo potenza\") %&gt;% as.numeric(),\n    `Campagnolo super record` = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"campagnolo super record\") %&gt;% as.numeric(),\n    `shimano nexus`           = `Shift Lever`     %&gt;% str_to_lower() %&gt;% str_detect(\"shimano nexus\") %&gt;% as.numeric(),\n    `shimano alfine`          = `Shift Lever`     %&gt;% str_to_lower() %&gt;% str_detect(\"shimano alfine\") %&gt;% as.numeric()\n  ) %&gt;%  \n  select(-c(`Rear Derailleur`, `Shift Lever`)) %&gt;% \n  mutate_if(is.numeric, ~replace(., is.na(.), 0)) \nbike_features_tbl &lt;- bike_features_tbl %&gt;% \n  mutate(id = row_number()) %&gt;% \n  mutate(frame_material = factor(frame_material)) %&gt;%\n  select(id, everything()) \nbike_features_tbl %&gt;% distinct(category_2)\n\n\n  \n\n\nsplit_obj &lt;- rsample::initial_split(bike_features_tbl, prop   = 0.80, \n                                    strata = \"category_2\")\nsplit_obj %&gt;% training() %&gt;% distinct(category_2)\n\n\n  \n\n\nsplit_obj %&gt;% testing() %&gt;% distinct(category_2)\n\n\n  \n\n\ntrain_tbl &lt;- training(split_obj)\ntest_tbl  &lt;- testing(split_obj)\ntrain_data &lt;- train_tbl %&gt;% set_names(str_replace_all(names(train_tbl), \" |-\", \"_\"))\ntest_data  &lt;- test_tbl  %&gt;% set_names(str_replace_all(names(test_tbl),  \" |-\", \"_\"))\n# recipe\nbike_rec &lt;- \n  recipe(frame_material ~ ., data = train_data) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes()) %&gt;% \n  step_zv(all_predictors()) \nd &lt;- summary(bike_rec)\nlr_mod &lt;- \n  logistic_reg() %&gt;% \n  set_engine(\"glm\")\nlr_mod\n\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glm\n\nbike_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lr_mod) %&gt;% \n  add_recipe(bike_rec)\nbike_wflow\n\n#&gt; ══ Workflow ════════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: logistic_reg()\n#&gt; \n#&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n#&gt; 2 Recipe Steps\n#&gt; \n#&gt; • step_dummy()\n#&gt; • step_zv()\n#&gt; \n#&gt; ── Model ───────────────────────────────────────────────────────────────────────\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glm\n\nbike_fit &lt;- \n  bike_wflow %&gt;% \n  fit(data = train_data)\n\n#&gt; Warning: glm.fit: algorithm did not converge\n\n\n#&gt; Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nbike_fit\n\n#&gt; ══ Workflow [trained] ══════════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: logistic_reg()\n#&gt; \n#&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n#&gt; 2 Recipe Steps\n#&gt; \n#&gt; • step_dummy()\n#&gt; • step_zv()\n#&gt; \n#&gt; ── Model ───────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;                (Intercept)                          id  \n#&gt;                 -9.059e+15                   9.678e+13  \n#&gt;                     weight                       price  \n#&gt;                 -7.069e+14                   6.701e+11  \n#&gt;           shimano_dura_ace             shimano_ultegra  \n#&gt;                 -2.062e+15                  -1.975e+15  \n#&gt;                shimano_105              shimano_tiagra  \n#&gt;                 -6.118e+14                  -2.488e+15  \n#&gt;              shimano_deore                 shimano_slx  \n#&gt;                  1.656e+15                  -2.753e+15  \n#&gt;                shimano_grx                  Shimano_xt  \n#&gt;                 -7.287e+14                   1.312e+14  \n#&gt;              Shimano_saint                    SRAM_red  \n#&gt;                  4.129e+15                  -4.311e+14  \n#&gt;                 SRAM_force                  SRAM_rival  \n#&gt;                 -4.445e+14                   3.380e+15  \n#&gt;                  SRAM_apex                    SRAM_xx1  \n#&gt;                 -4.151e+15                  -1.875e+14  \n#&gt;                   SRAM_x01                     SRAM_gx  \n#&gt;                 -1.828e+13                   6.694e+14  \n#&gt;                    SRAM_nx                     SRAM_sx  \n#&gt;                 -1.697e+13                  -1.997e+15  \n#&gt;         Campagnolo_potenza     Campagnolo_super_record  \n#&gt;                 -3.065e+15                  -2.225e+15  \n#&gt;              shimano_nexus              shimano_alfine  \n#&gt;                 -1.883e+15                   4.933e+14  \n#&gt;          category_1_Gravel    category_1_Hybrid...City  \n#&gt;                  6.192e+15                  -8.993e+15  \n#&gt;        category_1_Mountain             category_1_Road  \n#&gt;                  1.218e+15                   1.066e+16  \n#&gt;        category_2_All.Road             category_2_City  \n#&gt;                 -8.942e+14                   1.850e+15  \n#&gt;   category_2_Cross.Country       category_2_Cyclocross  \n#&gt;                  3.129e+15                  -3.359e+13  \n#&gt;       category_2_Dirt.Jump         category_2_Downhill  \n#&gt;                 -6.440e+15                  -2.876e+15  \n#&gt;          category_2_E.City        category_2_E.Fitness  \n#&gt;                  4.276e+15                  -5.199e+15  \n#&gt;        category_2_E.Gravel       category_2_E.Mountain  \n#&gt;                  1.121e+16                   7.332e+15  \n#&gt;          category_2_E.Road       category_2_E.Trekking  \n#&gt;                  1.498e+16                          NA  \n#&gt;       category_2_Endurance           category_2_Enduro  \n#&gt;                  2.287e+15                  -1.756e+15  \n#&gt;       category_2_Fat.Bikes             category_2_Race  \n#&gt;                  9.888e+15                   3.915e+15  \n#&gt; \n#&gt; ...\n#&gt; and 36 more lines.\n\nbike_fit %&gt;% \n  pull_workflow_fit() %&gt;% \n  tidy()\n\n#&gt; Warning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.\n#&gt; ℹ Please use `extract_fit_parsnip()` instead.\n\n\n\n  \n\n\nbike_pred &lt;- \n  predict(bike_fit, test_data, type=\"prob\") %&gt;% \n  bind_cols(test_data %&gt;% select(frame_material, category_2)) \n\n#&gt; Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\n#&gt; prediction from rank-deficient fit; attr(*, \"non-estim\") has doubtful cases\n\nbike_pred %&gt;% \n  roc_curve(truth = frame_material, .pred_aluminium) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\nbike_pred %&gt;% \n  roc_curve(truth = frame_material, .pred_carbon) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\nbike_pred\n\n\n  \n\n\nroc_al &lt;- bike_pred %&gt;% \n  roc_auc(truth = frame_material, .pred_aluminium)\nroc_al\n\n\n  \n\n\nroc_car &lt;- bike_pred %&gt;% \n  roc_auc(truth = frame_material, .pred_carbon)\nroc_car\n\n\n  \n\n\n# Evaluation\nmodel_01_linear_lm_simple &lt;- linear_reg(mode = \"regression\") %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(price ~ category_2 + frame_material, data = train_data)\nmodel_01_linear_lm_simple\n\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = price ~ category_2 + frame_material, data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;              (Intercept)        category_2All-Road            category_2City  \n#&gt;                  2593.63                   -756.23                  -1648.31  \n#&gt;  category_2Cross-Country      category_2Cyclocross       category_2Dirt Jump  \n#&gt;                   -29.52                   -904.11                  -1569.63  \n#&gt;       category_2Downhill          category_2E-City       category_2E-Fitness  \n#&gt;                   117.68                    477.07                    360.37  \n#&gt;       category_2E-Gravel      category_2E-Mountain          category_2E-Road  \n#&gt;                  1100.00                    778.12                    325.37  \n#&gt;     category_2E-Trekking       category_2Endurance          category_2Enduro  \n#&gt;                   731.08                  -1125.94                    223.26  \n#&gt;      category_2Fat Bikes            category_2Race         category_2Touring  \n#&gt;                 -1705.00                    681.92                  -1463.20  \n#&gt;          category_2Trail  category_2Triathlon Bike      frame_materialcarbon  \n#&gt;                  -691.98                    156.67                   1495.37\n\ntest_data &lt;- test_data %&gt;% filter(category_2 != \"Fat Bikes\")\nyards &lt;- model_01_linear_lm_simple\nyards\n\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = price ~ category_2 + frame_material, data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;              (Intercept)        category_2All-Road            category_2City  \n#&gt;                  2593.63                   -756.23                  -1648.31  \n#&gt;  category_2Cross-Country      category_2Cyclocross       category_2Dirt Jump  \n#&gt;                   -29.52                   -904.11                  -1569.63  \n#&gt;       category_2Downhill          category_2E-City       category_2E-Fitness  \n#&gt;                   117.68                    477.07                    360.37  \n#&gt;       category_2E-Gravel      category_2E-Mountain          category_2E-Road  \n#&gt;                  1100.00                    778.12                    325.37  \n#&gt;     category_2E-Trekking       category_2Endurance          category_2Enduro  \n#&gt;                   731.08                  -1125.94                    223.26  \n#&gt;      category_2Fat Bikes            category_2Race         category_2Touring  \n#&gt;                 -1705.00                    681.92                  -1463.20  \n#&gt;          category_2Trail  category_2Triathlon Bike      frame_materialcarbon  \n#&gt;                  -691.98                    156.67                   1495.37\n\ng1 &lt;- bike_features_tbl %&gt;% \n  mutate(category_2 = as.factor(category_2) %&gt;% \n           fct_reorder(price)) %&gt;% \n  \n  ggplot(aes(category_2, price)) +\n  geom_violin() +\n  geom_jitter(width = 0.1, alpha = 0.5, color = \"#FF00FF\") +\n  coord_flip() +\n  facet_wrap(~ frame_material) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  labs(\n    title = \"Unit Price for Each Model\",\n    y = \"\", x = \"Category 2\"\n  )\ng1\n\n#&gt; Warning: Groups with fewer than two datapoints have been dropped.\n#&gt; ℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\n#&gt; Warning: Groups with fewer than two datapoints have been dropped.\n#&gt; ℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n#&gt; Groups with fewer than two datapoints have been dropped.\n#&gt; ℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\n\n\n\n\n\n\nnew_race_alu &lt;- tibble(\n  model = \"Exceed AL SL new\",\n  category_2 = \"Race\",\n  frame_material = \"aluminium\",\n  shimano_dura_ace = 0,\n  shimano_ultegra = 0,\n  shimano_105 = 0,\n  shimano_tiagra = 0,\n  Shimano_sora = 0,\n  shimano_deore = 0,\n  shimano_slx = 0,\n  shimano_grx = 0,\n  Shimano_xt = 1,\n  Shimano_xtr = 0,\n  Shimano_saint = 0,\n  SRAM_red = 0,\n  SRAM_force = 0,\n  SRAM_rival = 0,\n  SRAM_apex = 0,\n  SRAM_xx1 = 0,\n  SRAM_x01 = 0,\n  SRAM_gx = 0,\n  SRAM_nx = 0,\n  SRAM_sx = 0,\n  Campagnolo_potenza = 0,\n  Campagnolo_super_record = 0,\n  shimano_nexus = 0,\n  shimano_alfine = 0\n) \nnew_race_alu\n\n\n  \n\n\npredict(model_01_linear_lm_simple, new_data = new_race_alu)\n\n\n  \n\n\nmodels_tbl &lt;- tibble(\n  model_id = str_c(\"Model 0\", 1:1),\n  model = list(\n    model_01_linear_lm_simple\n  )\n)\nmodels_tbl\n\n\n  \n\n\npredictions_new_race_alu_tbl &lt;- models_tbl %&gt;%\n  mutate(predictions = map(model, predict, new_data = new_race_alu)) %&gt;%\n  unnest(predictions) %&gt;%\n  mutate(category_2 = \"Race\") %&gt;%\n  left_join(new_race_alu, by = \"category_2\")\npredictions_new_race_alu_tbl\n\n\n  \n\n\ng2 &lt;- g1 +\n  geom_point(aes(y = .pred), color = \"red\", alpha = 0.5,\n             data = predictions_new_race_alu_tbl) +\n  ggrepel::geom_text_repel(aes(label = model_id, y = .pred),\n                           size = 3,\n                           data = predictions_new_race_alu_tbl)\ng2\n\n#&gt; Warning: Groups with fewer than two datapoints have been dropped.\n#&gt; ℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n#&gt; Groups with fewer than two datapoints have been dropped.\n#&gt; ℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n#&gt; Groups with fewer than two datapoints have been dropped.\n#&gt; ℹ Set `drop = FALSE` to consider such groups for position adjustment purposes."
  },
  {
    "objectID": "content/01_journal/01_Machine_learning_fundamentals.html",
    "href": "content/01_journal/01_Machine_learning_fundamentals.html",
    "title": "01 Machine Learning Fundamentals",
    "section": "",
    "text": "Load the absolute path to the data directory.\ndata_dir &lt;- params$data_dir"
  },
  {
    "objectID": "content/01_journal/01_Machine_learning_fundamentals.html#stock-prices-standardization",
    "href": "content/01_journal/01_Machine_learning_fundamentals.html#stock-prices-standardization",
    "title": "01 Machine Learning Fundamentals",
    "section": "2.1 Stock Prices Standardization",
    "text": "2.1 Stock Prices Standardization\nStock prices (adjusted stock price) are standardized by converting them into daily returns (percent change from previous day). This is done such that the stock prices are of the same magnitude and can thus be compared. Below is the sp 500 price table shown:\n\nsp_500_prices_tbl %&gt;% glimpse()\n\n#&gt; Rows: 1,225,765\n#&gt; Columns: 8\n#&gt; $ symbol   &lt;chr&gt; \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT…\n#&gt; $ date     &lt;date&gt; 2009-01-02, 2009-01-05, 2009-01-06, 2009-01-07, 2009-01-08, …\n#&gt; $ open     &lt;dbl&gt; 19.53, 20.20, 20.75, 20.19, 19.63, 20.17, 19.71, 19.52, 19.53…\n#&gt; $ high     &lt;dbl&gt; 20.40, 20.67, 21.00, 20.29, 20.19, 20.30, 19.79, 19.99, 19.68…\n#&gt; $ low      &lt;dbl&gt; 19.37, 20.06, 20.61, 19.48, 19.55, 19.41, 19.30, 19.52, 19.01…\n#&gt; $ close    &lt;dbl&gt; 20.33, 20.52, 20.76, 19.51, 20.12, 19.52, 19.47, 19.82, 19.09…\n#&gt; $ volume   &lt;dbl&gt; 50084000, 61475200, 58083400, 72709900, 70255400, 49815300, 5…\n#&gt; $ adjusted &lt;dbl&gt; 15.86624, 16.01451, 16.20183, 15.22628, 15.70234, 15.23408, 1…\n\n\n\nsp_500_daily_returns_tbl &lt;- sp_500_prices_tbl %&gt;%\n  select(symbol, date, adjusted) %&gt;%\n  filter(date &gt;= as.Date(\"2018-01-01\")) %&gt;%\n  group_by(symbol) %&gt;%\n  mutate(adjusted_lag = lag(adjusted)) %&gt;%\n  filter(!is.na(adjusted_lag)) %&gt;%\n  mutate(difference = adjusted - adjusted_lag) %&gt;%\n  mutate(pct_return = difference / adjusted_lag) %&gt;%\n  select(symbol, date, pct_return) %&gt;%\n  ungroup()\nprint(sp_500_daily_returns_tbl)\n\n#&gt; # A tibble: 141,340 × 3\n#&gt;    symbol date       pct_return\n#&gt;    &lt;chr&gt;  &lt;date&gt;          &lt;dbl&gt;\n#&gt;  1 MSFT   2018-01-03   0.00465 \n#&gt;  2 MSFT   2018-01-04   0.00880 \n#&gt;  3 MSFT   2018-01-05   0.0124  \n#&gt;  4 MSFT   2018-01-08   0.00102 \n#&gt;  5 MSFT   2018-01-09  -0.000680\n#&gt;  6 MSFT   2018-01-10  -0.00453 \n#&gt;  7 MSFT   2018-01-11   0.00296 \n#&gt;  8 MSFT   2018-01-12   0.0173  \n#&gt;  9 MSFT   2018-01-16  -0.0140  \n#&gt; 10 MSFT   2018-01-17   0.0203  \n#&gt; # ℹ 141,330 more rows"
  },
  {
    "objectID": "content/01_journal/01_Machine_learning_fundamentals.html#conversion-to-user-item-format",
    "href": "content/01_journal/01_Machine_learning_fundamentals.html#conversion-to-user-item-format",
    "title": "01 Machine Learning Fundamentals",
    "section": "2.2 Conversion to User-Item Format",
    "text": "2.2 Conversion to User-Item Format\nThe next step involves converting to a user-item format with the symbol in the first column and every other column the value of the daily returns (pct_return) for every stock at each date. The user in this case is the symbol (company), and the item in this case is the pct_return at each date.\nImporting the correct results first (just in case I was not able to complete the last step).\n\nsp_500_daily_returns_tbl &lt;- read_rds(\"C:/Users/risho/OneDrive/Desktop/internship_sparks/ss24-bdml-rishon1234/ML/data/Business Decisions with Machine Learning/sp_500_daily_returns_tbl.rds\")\nprint(sp_500_daily_returns_tbl)\n\n#&gt; # A tibble: 141,340 × 3\n#&gt;    symbol date       pct_return\n#&gt;    &lt;chr&gt;  &lt;date&gt;          &lt;dbl&gt;\n#&gt;  1 MSFT   2018-01-03   0.00465 \n#&gt;  2 MSFT   2018-01-04   0.00880 \n#&gt;  3 MSFT   2018-01-05   0.0124  \n#&gt;  4 MSFT   2018-01-08   0.00102 \n#&gt;  5 MSFT   2018-01-09  -0.000680\n#&gt;  6 MSFT   2018-01-10  -0.00453 \n#&gt;  7 MSFT   2018-01-11   0.00296 \n#&gt;  8 MSFT   2018-01-12   0.0173  \n#&gt;  9 MSFT   2018-01-16  -0.0140  \n#&gt; 10 MSFT   2018-01-17   0.0203  \n#&gt; # ℹ 141,330 more rows\n\n\nAnd the conversion follows with:\n\nstock_date_matrix_tbl &lt;- sp_500_daily_returns_tbl %&gt;%\n  spread(key = date, value = pct_return, fill = 0)\nstock_date_matrix_tbl |&gt; as_tibble() |&gt; print()\n\n#&gt; # A tibble: 502 × 283\n#&gt;    symbol `2018-01-03` `2018-01-04` `2018-01-05` `2018-01-08` `2018-01-09`\n#&gt;    &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n#&gt;  1 A          0.0254       -0.00750     0.0160        0.00215     0.0246  \n#&gt;  2 AAL       -0.0123        0.00630    -0.000380     -0.00988    -0.000959\n#&gt;  3 AAP        0.00905       0.0369      0.0106       -0.00704    -0.00808 \n#&gt;  4 AAPL      -0.000174      0.00465     0.0114       -0.00371    -0.000115\n#&gt;  5 ABBV       0.0156       -0.00570     0.0174       -0.0160      0.00754 \n#&gt;  6 ABC        0.00372      -0.00222     0.0121        0.0166      0.00640 \n#&gt;  7 ABMD       0.0173        0.0175      0.0154        0.0271      0.00943 \n#&gt;  8 ABT        0.00221      -0.00170     0.00289      -0.00288     0.00170 \n#&gt;  9 ACN        0.00462       0.0118      0.00825       0.00799     0.00333 \n#&gt; 10 ADBE       0.0188        0.0120      0.0116       -0.00162     0.00897 \n#&gt; # ℹ 492 more rows\n#&gt; # ℹ 277 more variables: `2018-01-10` &lt;dbl&gt;, `2018-01-11` &lt;dbl&gt;,\n#&gt; #   `2018-01-12` &lt;dbl&gt;, `2018-01-16` &lt;dbl&gt;, `2018-01-17` &lt;dbl&gt;,\n#&gt; #   `2018-01-18` &lt;dbl&gt;, `2018-01-19` &lt;dbl&gt;, `2018-01-22` &lt;dbl&gt;,\n#&gt; #   `2018-01-23` &lt;dbl&gt;, `2018-01-24` &lt;dbl&gt;, `2018-01-25` &lt;dbl&gt;,\n#&gt; #   `2018-01-26` &lt;dbl&gt;, `2018-01-29` &lt;dbl&gt;, `2018-01-30` &lt;dbl&gt;,\n#&gt; #   `2018-01-31` &lt;dbl&gt;, `2018-02-01` &lt;dbl&gt;, `2018-02-02` &lt;dbl&gt;, …"
  },
  {
    "objectID": "content/01_journal/01_Machine_learning_fundamentals.html#k-means-clustering",
    "href": "content/01_journal/01_Machine_learning_fundamentals.html#k-means-clustering",
    "title": "01 Machine Learning Fundamentals",
    "section": "2.3 K-Means Clustering",
    "text": "2.3 K-Means Clustering\nImporting the correct results first (just in case I was not able to complete the last step).\n\nstock_date_matrix_tbl &lt;- read_rds(\"C:/Users/risho/OneDrive/Desktop/internship_sparks/ss24-bdml-rishon1234/ML/data/Business Decisions with Machine Learning/stock_date_matrix_tbl.rds\")\nstock_date_matrix_tbl |&gt; as_tibble() |&gt; print()\n\n#&gt; # A tibble: 502 × 283\n#&gt;    symbol `2018-01-03` `2018-01-04` `2018-01-05` `2018-01-08` `2018-01-09`\n#&gt;    &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n#&gt;  1 A          0.0254       -0.00750     0.0160        0.00215     0.0246  \n#&gt;  2 AAL       -0.0123        0.00630    -0.000380     -0.00988    -0.000959\n#&gt;  3 AAP        0.00905       0.0369      0.0106       -0.00704    -0.00808 \n#&gt;  4 AAPL      -0.000174      0.00465     0.0114       -0.00371    -0.000115\n#&gt;  5 ABBV       0.0156       -0.00570     0.0174       -0.0160      0.00754 \n#&gt;  6 ABC        0.00372      -0.00222     0.0121        0.0166      0.00640 \n#&gt;  7 ABMD       0.0173        0.0175      0.0154        0.0271      0.00943 \n#&gt;  8 ABT        0.00221      -0.00170     0.00289      -0.00288     0.00170 \n#&gt;  9 ACN        0.00462       0.0118      0.00825       0.00799     0.00333 \n#&gt; 10 ADBE       0.0188        0.0120      0.0116       -0.00162     0.00897 \n#&gt; # ℹ 492 more rows\n#&gt; # ℹ 277 more variables: `2018-01-10` &lt;dbl&gt;, `2018-01-11` &lt;dbl&gt;,\n#&gt; #   `2018-01-12` &lt;dbl&gt;, `2018-01-16` &lt;dbl&gt;, `2018-01-17` &lt;dbl&gt;,\n#&gt; #   `2018-01-18` &lt;dbl&gt;, `2018-01-19` &lt;dbl&gt;, `2018-01-22` &lt;dbl&gt;,\n#&gt; #   `2018-01-23` &lt;dbl&gt;, `2018-01-24` &lt;dbl&gt;, `2018-01-25` &lt;dbl&gt;,\n#&gt; #   `2018-01-26` &lt;dbl&gt;, `2018-01-29` &lt;dbl&gt;, `2018-01-30` &lt;dbl&gt;,\n#&gt; #   `2018-01-31` &lt;dbl&gt;, `2018-02-01` &lt;dbl&gt;, `2018-02-02` &lt;dbl&gt;, …\n\n\nAnd then executing the KMeans operation:\n\n# Create kmeans_obj for 4 centers\nNUM_CENTERS &lt;- 4\nN_START = 20\n\nkmeans_obj &lt;- stock_date_matrix_tbl %&gt;%\n    select(-symbol) %&gt;%\n    kmeans(centers = NUM_CENTERS, nstart = N_START)\nprint(kmeans_obj$cluster)\n\n#&gt;   [1] 1 2 1 2 1 1 2 1 1 2 2 1 1 1 2 3 3 3 1 1 1 3 1 1 2 1 2 1 1 1 2 2 2 1 1 1 1\n#&gt;  [38] 3 2 2 2 1 1 1 4 4 1 1 1 3 1 3 2 3 2 1 3 1 1 1 1 1 1 1 1 1 1 4 1 1 1 1 1 1\n#&gt;  [75] 1 1 1 3 1 3 1 1 1 1 1 1 3 2 1 1 1 1 1 3 1 1 1 1 3 3 1 1 1 1 1 3 1 3 1 1 1\n#&gt; [112] 4 1 1 3 1 1 2 2 1 1 1 1 1 1 4 4 3 1 1 1 1 1 1 1 1 1 1 1 3 1 1 3 1 3 3 1 4\n#&gt; [149] 1 1 2 1 1 3 1 3 1 1 1 4 3 3 3 3 1 1 3 3 2 3 1 1 3 1 4 1 2 1 4 1 3 2 1 1 1\n#&gt; [186] 1 1 4 1 1 1 1 1 1 3 4 2 1 1 1 1 3 1 1 2 2 1 2 1 1 1 1 1 4 1 1 1 1 3 1 4 4\n#&gt; [223] 1 1 1 1 1 1 4 2 1 1 3 1 1 1 3 1 1 1 2 1 2 2 1 2 2 1 1 2 1 1 3 2 1 1 1 1 1\n#&gt; [260] 1 1 1 1 1 1 1 3 1 2 3 3 2 3 4 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 2 1 3 1\n#&gt; [297] 1 2 3 3 1 1 2 3 2 1 1 3 1 1 2 1 3 1 1 1 1 3 1 4 1 4 1 2 2 1 1 1 2 2 1 4 1\n#&gt; [334] 1 3 1 2 3 1 2 1 1 4 3 1 2 1 1 2 1 1 1 3 4 1 1 1 4 1 1 1 3 3 1 1 3 1 1 1 1\n#&gt; [371] 1 3 3 1 1 3 1 3 1 1 3 4 1 1 4 2 1 2 1 1 3 1 1 2 1 1 1 1 1 1 1 1 1 1 3 1 1\n#&gt; [408] 1 1 1 3 4 3 1 2 3 3 1 3 1 1 2 1 1 2 1 1 2 3 1 3 1 1 1 1 1 1 1 1 1 2 1 1 1\n#&gt; [445] 3 2 2 2 2 1 2 2 1 3 1 1 1 1 1 1 2 1 1 2 1 1 1 4 1 3 1 2 2 3 3 1 1 1 2 3 3\n#&gt; [482] 1 1 1 3 4 1 1 1 3 2 4 3 2 4 1 1 1 1 1 1 1\n\n\nAnd using glance() to get the tot.withinss.\n\nkmeans_obj %&gt;% glance() %&gt;% print()\n\n#&gt; # A tibble: 1 × 4\n#&gt;   totss tot.withinss betweenss  iter\n#&gt;   &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n#&gt; 1  33.6         29.2      4.40     4"
  },
  {
    "objectID": "content/01_journal/01_Machine_learning_fundamentals.html#finding-optimal-k",
    "href": "content/01_journal/01_Machine_learning_fundamentals.html#finding-optimal-k",
    "title": "01 Machine Learning Fundamentals",
    "section": "2.4 Finding Optimal K",
    "text": "2.4 Finding Optimal K\n\nkmeans_mapper &lt;- function(center = 3) {\n    stock_date_matrix_tbl %&gt;%\n        select(-symbol) %&gt;%\n        kmeans(centers = center, nstart = 20)\n}\n\n\n# Use purrr to map\nkmeans_mapped_tbl &lt;- tibble(centers = 1:30) %&gt;%\n    mutate(k_means = centers %&gt;% map(kmeans_mapper)) %&gt;%\n    mutate(glance  = k_means %&gt;% map(glance))\nprint(kmeans_mapped_tbl)\n\n#&gt; # A tibble: 30 × 3\n#&gt;    centers k_means  glance          \n#&gt;      &lt;int&gt; &lt;list&gt;   &lt;list&gt;          \n#&gt;  1       1 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  2       2 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  3       3 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  4       4 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  5       5 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  6       6 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  7       7 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  8       8 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  9       9 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt; 10      10 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt; # ℹ 20 more rows\n\n\n\n# Visualize Scree Plot\nkmeans_mapped_tbl %&gt;%\n    unnest(glance) %&gt;%\n    ggplot(aes(x = centers, y = tot.withinss)) +\n    geom_point() +\n    geom_line()"
  },
  {
    "objectID": "content/01_journal/01_Machine_learning_fundamentals.html#umap-application",
    "href": "content/01_journal/01_Machine_learning_fundamentals.html#umap-application",
    "title": "01 Machine Learning Fundamentals",
    "section": "2.5 UMAP Application",
    "text": "2.5 UMAP Application\n\nk_means_mapped_tbl &lt;- read_rds(\"C:/Users/risho/OneDrive/Desktop/internship_sparks/ss24-bdml-rishon1234/ML/data/Business Decisions with Machine Learning/k_means_mapped_tbl.rds\")\nprint(k_means_mapped_tbl)\n\n#&gt; # A tibble: 30 × 3\n#&gt;    centers k_means  glance          \n#&gt;      &lt;int&gt; &lt;list&gt;   &lt;list&gt;          \n#&gt;  1       1 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  2       2 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  3       3 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  4       4 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  5       5 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  6       6 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  7       7 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  8       8 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  9       9 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt; 10      10 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt; # ℹ 20 more rows\n\n\n\n# Apply UMAP\numap_results &lt;- stock_date_matrix_tbl %&gt;%\n  select(-symbol) %&gt;%\n  umap()\numap_results\n\n#&gt; umap embedding of 502 items in 2 dimensions\n#&gt; object components: layout, data, knn, config\n\n\n\n# Convert umap results to tibble with symbols\numap_results_tbl &lt;- umap_results$layout %&gt;%\n    as_tibble() %&gt;%\n    bind_cols(\n      stock_date_matrix_tbl %&gt;% select(symbol)\n  )\n\n#&gt; Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n#&gt; `.name_repair` is omitted as of tibble 2.0.0.\n#&gt; ℹ Using compatibility `.name_repair`.\n\nprint(umap_results_tbl)\n\n#&gt; # A tibble: 502 × 3\n#&gt;         V1      V2 symbol\n#&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; \n#&gt;  1 -1.56    0.148  A     \n#&gt;  2 -0.351   2.50   AAL   \n#&gt;  3 -0.0288 -1.18   AAP   \n#&gt;  4 -2.97   -0.641  AAPL  \n#&gt;  5  0.109   0.227  ABBV  \n#&gt;  6  0.550  -0.406  ABC   \n#&gt;  7 -2.92   -0.836  ABMD  \n#&gt;  8 -1.23   -0.0930 ABT   \n#&gt;  9 -1.66   -0.527  ACN   \n#&gt; 10 -2.98   -1.08   ADBE  \n#&gt; # ℹ 492 more rows\n\n\n\n# Visualize UMAP results\numap_results_tbl %&gt;%\n  ggplot(aes(x = V1, y = V2)) +\n  geom_point(alpha = 0.5) +\n  theme_tq() +\n  labs(title = \"UMAP Projection\")"
  },
  {
    "objectID": "content/01_journal/01_Machine_learning_fundamentals.html#combination-of-k-means-and-umap",
    "href": "content/01_journal/01_Machine_learning_fundamentals.html#combination-of-k-means-and-umap",
    "title": "01 Machine Learning Fundamentals",
    "section": "2.6 Combination of K-Means and UMAP",
    "text": "2.6 Combination of K-Means and UMAP\nNow the K-Means clusters and the UMAP 2D representation are being combined\nImporting the correct results first (just in case I was not able to complete the last step).\n\nk_means_mapped_tbl &lt;- read_rds(\"C:/Users/risho/OneDrive/Desktop/internship_sparks/ss24-bdml-rishon1234/ML/data/Business Decisions with Machine Learning/k_means_mapped_tbl.rds\")\numap_results_tbl   &lt;- read_rds(\"C:/Users/risho/OneDrive/Desktop/internship_sparks/ss24-bdml-rishon1234/ML/data/Business Decisions with Machine Learning/umap_results_tbl.rds\")\nprint(umap_results_tbl)\n\n#&gt; # A tibble: 502 × 3\n#&gt;         V1      V2 symbol\n#&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; \n#&gt;  1 -0.764   1.65   A     \n#&gt;  2 -2.70    0.455  AAL   \n#&gt;  3  0.739  -0.0320 AAP   \n#&gt;  4  0.0130  3.09   AAPL  \n#&gt;  5 -0.965  -0.0193 ABBV  \n#&gt;  6 -0.506  -0.659  ABC   \n#&gt;  7  0.436   3.10   ABMD  \n#&gt;  8 -0.262   1.35   ABT   \n#&gt;  9  0.0598  1.63   ACN   \n#&gt; 10  0.570   3.43   ADBE  \n#&gt; # ℹ 492 more rows\n\n\nNow, the first 10 KMeans items are to be selected as the ScreePlot flattens beyond this one.\n\n# Get the k_means_obj from the 10th center\nk_means_obj &lt;- k_means_mapped_tbl %&gt;%\n  pull(k_means) %&gt;%\n  pluck(10)\n\nNext, the clusters from the k_means_obj with the umap_results_tbl are being combined.\n\numap_kmeans_results_tbl &lt;- k_means_obj %&gt;%\n  augment(stock_date_matrix_tbl) %&gt;%\n  select(symbol, .cluster) %&gt;%\n  left_join(umap_results_tbl, by = \"symbol\") %&gt;%\n  left_join(sp_500_index_tbl %&gt;% select(symbol, company, sector), by = \"symbol\")\nprint(umap_kmeans_results_tbl)\n\n#&gt; # A tibble: 502 × 6\n#&gt;    symbol .cluster      V1      V2 company                       sector         \n#&gt;    &lt;chr&gt;  &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                         &lt;chr&gt;          \n#&gt;  1 A      7        -0.764   1.65   Agilent Technologies Inc.     Health Care    \n#&gt;  2 AAL    2        -2.70    0.455  American Airlines Group Inc.  Industrials    \n#&gt;  3 AAP    10        0.739  -0.0320 Advance Auto Parts Inc.       Consumer Discr…\n#&gt;  4 AAPL   9         0.0130  3.09   Apple Inc.                    Information Te…\n#&gt;  5 ABBV   7        -0.965  -0.0193 AbbVie Inc.                   Health Care    \n#&gt;  6 ABC    5        -0.506  -0.659  AmerisourceBergen Corporation Health Care    \n#&gt;  7 ABMD   9         0.436   3.10   ABIOMED Inc.                  Health Care    \n#&gt;  8 ABT    7        -0.262   1.35   Abbott Laboratories           Health Care    \n#&gt;  9 ACN    7         0.0598  1.63   Accenture Plc Class A         Information Te…\n#&gt; 10 ADBE   9         0.570   3.43   Adobe Inc.                    Information Te…\n#&gt; # ℹ 492 more rows\n\n\nAnd finally plotting the K-Means and UMAP results.\n\n# Visualize the combined K-Means and UMAP results\nlibrary(viridis)\n\n#&gt; Loading required package: viridisLite\n\numap_kmeans_results_tbl %&gt;%\n  ggplot(aes(x = V1, y = V2, color = .cluster)) +\n  geom_point(alpha = 0.5) +\n  scale_color_manual(values = viridis_pal()(10))"
  },
  {
    "objectID": "content/01_journal/03_Automated_ML_with_H2o.html",
    "href": "content/01_journal/03_Automated_ML_with_H2o.html",
    "title": "03 Automated Machine Learning with H20",
    "section": "",
    "text": "Load the absolute path to the data directory.\n\ndata_dir &lt;- params$data_dir\n\n\n1 Libraries\n\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(readxl)\nlibrary(skimr)\nlibrary(GGally)\n\n#&gt; Registered S3 method overwritten by 'GGally':\n#&gt;   method from   \n#&gt;   +.gg   ggplot2\n\n\n\n\n2 Load Data Definitions\n\ndefinitions_raw_tbl   &lt;- read_excel(file.path(\"C:/Users/risho/OneDrive/Desktop/internship_sparks/ss24-bdml-rishon1234/ML/data/Business Decisions with Machine Learning/data_definitions.xlsx\"), sheet = 1, col_names = FALSE)\n\n#&gt; New names:\n#&gt; • `` -&gt; `...1`\n#&gt; • `` -&gt; `...2`\n\ndefinitions_raw_tbl |&gt; as_tibble() |&gt; print()\n\n#&gt; # A tibble: 35 × 2\n#&gt;    ...1                    ...2             \n#&gt;    &lt;chr&gt;                   &lt;chr&gt;            \n#&gt;  1 Education               1 'Below College'\n#&gt;  2 &lt;NA&gt;                    2 'College'      \n#&gt;  3 &lt;NA&gt;                    3 'Bachelor'     \n#&gt;  4 &lt;NA&gt;                    4 'Master'       \n#&gt;  5 &lt;NA&gt;                    5 'Doctor'       \n#&gt;  6 &lt;NA&gt;                    &lt;NA&gt;             \n#&gt;  7 EnvironmentSatisfaction 1 'Low'          \n#&gt;  8 &lt;NA&gt;                    2 'Medium'       \n#&gt;  9 &lt;NA&gt;                    3 'High'         \n#&gt; 10 &lt;NA&gt;                    4 'Very High'    \n#&gt; # ℹ 25 more rows\n\n\n\n\n3 Load Employee Attrition\n\nemployee_attrition_tbl &lt;- read_csv(\"C:/Users/risho/OneDrive/Desktop/internship_sparks/ss24-bdml-rishon1234/ML/data/Business Decisions with Machine Learning/datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n\n#&gt; Rows: 1470 Columns: 35\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr  (9): Attrition, BusinessTravel, Department, EducationField, Gender, Job...\n#&gt; dbl (26): Age, DailyRate, DistanceFromHome, Education, EmployeeCount, Employ...\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nemployee_attrition_tbl |&gt; as_tibble() |&gt; print()\n\n#&gt; # A tibble: 1,470 × 35\n#&gt;      Age Attrition BusinessTravel    DailyRate Department       DistanceFromHome\n#&gt;    &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;\n#&gt;  1    41 Yes       Travel_Rarely          1102 Sales                           1\n#&gt;  2    49 No        Travel_Frequently       279 Research & Deve…                8\n#&gt;  3    37 Yes       Travel_Rarely          1373 Research & Deve…                2\n#&gt;  4    33 No        Travel_Frequently      1392 Research & Deve…                3\n#&gt;  5    27 No        Travel_Rarely           591 Research & Deve…                2\n#&gt;  6    32 No        Travel_Frequently      1005 Research & Deve…                2\n#&gt;  7    59 No        Travel_Rarely          1324 Research & Deve…                3\n#&gt;  8    30 No        Travel_Rarely          1358 Research & Deve…               24\n#&gt;  9    38 No        Travel_Frequently       216 Research & Deve…               23\n#&gt; 10    36 No        Travel_Rarely          1299 Research & Deve…               27\n#&gt; # ℹ 1,460 more rows\n#&gt; # ℹ 29 more variables: Education &lt;dbl&gt;, EducationField &lt;chr&gt;,\n#&gt; #   EmployeeCount &lt;dbl&gt;, EmployeeNumber &lt;dbl&gt;, EnvironmentSatisfaction &lt;dbl&gt;,\n#&gt; #   Gender &lt;chr&gt;, HourlyRate &lt;dbl&gt;, JobInvolvement &lt;dbl&gt;, JobLevel &lt;dbl&gt;,\n#&gt; #   JobRole &lt;chr&gt;, JobSatisfaction &lt;dbl&gt;, MaritalStatus &lt;chr&gt;,\n#&gt; #   MonthlyIncome &lt;dbl&gt;, MonthlyRate &lt;dbl&gt;, NumCompaniesWorked &lt;dbl&gt;,\n#&gt; #   Over18 &lt;chr&gt;, OverTime &lt;chr&gt;, PercentSalaryHike &lt;dbl&gt;, …\n\n\n\n\n4 Create Visualization Method\n\nplot_ggpairs &lt;- function(data, color = NULL, density_alpha = 0.5) {\n    \n    color_expr &lt;- enquo(color)\n    \n    if (rlang::quo_is_null(color_expr)) {\n        \n        g &lt;- data %&gt;%\n            ggpairs(lower = \"blank\") \n        \n    } else {\n        \n        color_name &lt;- quo_name(color_expr)\n        \n        g &lt;- data %&gt;%\n            ggpairs(mapping = aes_string(color = color_name), \n                    lower = \"blank\", legend = 1,\n                    diag = list(continuous = wrap(\"densityDiag\", \n                                                  alpha = density_alpha))) +\n            theme(legend.position = \"bottom\")\n    }\n    \n    return(g)\n    \n}\n\n\n\n5 Visualize Feature Groups\n\n# Descriptive Features\nemployee_attrition_tbl %&gt;% \n  select(Attrition, Age, DistanceFromHome, Gender, MaritalStatus, NumCompaniesWorked, Over18) %&gt;%\n  plot_ggpairs(Attrition)\n\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`.\n#&gt; ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\n\n\n\n\n\n\n# Employment Features\nemployee_attrition_tbl %&gt;% \n  select(Attrition, Department, EmployeeCount, EmployeeNumber, JobInvolvement, JobLevel, JobRole, JobSatisfaction) %&gt;%\n  plot_ggpairs(Attrition)\n\n#&gt; Warning in cor(x, y): the standard deviation is zero\n\n\n#&gt; Warning in cor(x, y): the standard deviation is zero\n#&gt; Warning in cor(x, y): the standard deviation is zero\n#&gt; Warning in cor(x, y): the standard deviation is zero\n#&gt; Warning in cor(x, y): the standard deviation is zero\n#&gt; Warning in cor(x, y): the standard deviation is zero\n#&gt; Warning in cor(x, y): the standard deviation is zero\n#&gt; Warning in cor(x, y): the standard deviation is zero\n#&gt; Warning in cor(x, y): the standard deviation is zero\n#&gt; Warning in cor(x, y): the standard deviation is zero\n#&gt; Warning in cor(x, y): the standard deviation is zero\n#&gt; Warning in cor(x, y): the standard deviation is zero\n\n\n\n\n\n\n\n\n# Compensation Features\nemployee_attrition_tbl %&gt;% \n  select(Attrition, MonthlyIncome, PercentSalaryHike, StockOptionLevel) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n# Survery Results\nemployee_attrition_tbl %&gt;% \n  select(Attrition, EnvironmentSatisfaction, JobSatisfaction, RelationshipSatisfaction, WorkLifeBalance) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n# Performance Data\nemployee_attrition_tbl %&gt;% \n  select(Attrition, JobInvolvement, PerformanceRating) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n# Work-Life Features\nemployee_attrition_tbl %&gt;% \n  select(Attrition, BusinessTravel, OverTime) %&gt;%\n    plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n# Training & Education\nemployee_attrition_tbl %&gt;% \n  select(Attrition, Education, EducationField, TrainingTimesLastYear) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n# Time-Based Features\nemployee_attrition_tbl %&gt;% \n  select(Attrition, TotalWorkingYears, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager) %&gt;%\n    plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n\n\n\n6 Challenge Solutions\nThe challenge question are solved based on the graphs plotted above for the feature groups.\n\n1. Compensation Features\nWhat can you deduce about the interaction between Monthly Income and Attrition?\nSolution: c. Those that are leaving have a lower Monthly Income.\n\n\n2. Compensation Features\nWhat can you deduce about the interaction between Percent Salary Hike and Attrition?\nSolution: c. Those that are leaving have lower Percent Salary Hike\nNote: However, it must be noted this distinction is not super clear as for both attrition options “yes” and “no” the mean percent salary hike is the same. Only the max value of the percent salary hike is bigger for “yes”. That’s why I decide for c).\n\n\n3. Compensation Features\nWhat can you deduce about the interaction between Stock Option Level and Attrition?\nSolution: b. Those that are staying have a higher stock option level.\n\n\n4. Survey Results\nWhat can you deduce about the interaction between Environment Satisfaction and Attrition?\nSolution: A higher proportion of those leaving have a low environment satisfaction level.\n\n\n5. Survey Results\nWhat can you deduce about the interaction between Work Life Balance and Attrition\nSolution: b. Those that are staying have a higher density of 2’s and 3’s.\n\n\n6. Performance Data\nWhat Can you deduce about the interaction between Job Involvement and Attrition?\nSolution: a. Those that are leaving have a lower density of 3’s and 4’s.\n\n\n7. Work-Life Features\nWhat can you deduce about the interaction between Over Time and Attrition?\nSolution: b. The proportion of those staying that are working Over Time are high compared to those that are not staying.\n\n\n8. Training and Education\nWhat can you deduce about the interaction between Training Times Last Year and Attrition\nSolution: b. People that leave tend to have less annual trainings.\n\n\n9. Time-Based Features\nWhat can you deduce about the interaction between Years At Company and Attrition\nSolution: b. People that leave tend to have less working years at the company.\n\n\n10. Time-Based Features\nWhat can you deduce about the interaction between Years Since Last Promotion and Attrition?\nSolution: a. Those that are leaving have more years since last promotion than those that are staying."
  },
  {
    "objectID": "content/01_journal/06_deep_learning.html",
    "href": "content/01_journal/06_deep_learning.html",
    "title": "06 Deep Learning",
    "section": "",
    "text": "IMPORTANT: You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.\nThis is an .Rmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header."
  },
  {
    "objectID": "content/01_journal/06_deep_learning.html#second-level-header",
    "href": "content/01_journal/06_deep_learning.html#second-level-header",
    "title": "06 Deep Learning",
    "section": "1.1 Second level header",
    "text": "1.1 Second level header\nYou can add more headers by adding more hashtags. These won’t be put into the table of contents\n\nthird level header\nHere’s an even lower level header"
  }
]