[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website everytime before you want to upload changes"
  },
  {
    "objectID": "content/01_journal/02_supervised_ml.html",
    "href": "content/01_journal/02_supervised_ml.html",
    "title": "02 Supervised ML",
    "section": "",
    "text": "library(tidymodels)\nlibrary(broom.mixed)\nlibrary(xgboost)\nlibrary(tidyverse)\nlibrary(parsnip)\nlibrary(recipes)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(rpart.plot)\nlibrary(modeldata)\n# Data exploration\nbike_data_tbl &lt;- readRDS(\"C:/Users/risho/OneDrive/Desktop/internship_sparks/ss24-bdml-rishon1234/ML/data/Business Decisions with Machine Learning/bike_orderlines.rds\")\nmodel_sales_tbl &lt;- bike_data_tbl %&gt;%\n  select(total_price, model, category_2, frame_material) %&gt;%\n  \n  group_by(model, category_2, frame_material) %&gt;%\n  summarise(total_sales = sum(total_price)) %&gt;%\n  ungroup() %&gt;%\n  \n  arrange(desc(total_sales))\n\n#&gt; `summarise()` has grouped output by 'model', 'category_2'. You can override\n#&gt; using the `.groups` argument.\n\nmodel_sales_tbl %&gt;%\n  mutate(category_2 = as_factor(category_2) %&gt;% \n           fct_reorder(total_sales, .fun = max) %&gt;% \n           fct_rev()) %&gt;%\n  \n  ggplot(aes(frame_material, total_sales)) +\n  geom_violin() +\n  geom_jitter(width = 0.1, alpha = 0.5, color = \"#800080\") +\n  #coord_flip() +\n  facet_wrap(~ category_2) +\n  scale_y_continuous(labels = scales::dollar_format(scale = 1e-6, suffix = \"M\", accuracy = 0.1)) +\n  tidyquant::theme_tq() +\n  labs(\n    title = \"Total Sales for Each Model\",\n    x = \"Frame Material\", y = \"Revenue\"\n  )\n\n#&gt; Registered S3 method overwritten by 'quantmod':\n#&gt;   method            from\n#&gt;   as.zoo.data.frame zoo\n\n\n#&gt; Warning: Groups with fewer than two datapoints have been dropped.\n#&gt; ℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n#&gt; Groups with fewer than two datapoints have been dropped.\n#&gt; ℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n#&gt; Groups with fewer than two datapoints have been dropped.\n#&gt; ℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\n#&gt; Warning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\n#&gt; returning -Inf\n\n\n#&gt; Warning: Computation failed in `stat_ydensity()`.\n#&gt; Caused by error in `$&lt;-.data.frame`:\n#&gt; ! replacement has 1 row, data has 0\n\n\n\n\n\n\n\nbike_features_tbl &lt;- readRDS(\"C:/Users/risho/OneDrive/Desktop/internship_sparks/ss24-bdml-rishon1234/ML/data/Business Decisions with Machine Learning/bike_features_tbl.rds\")\nbike_features_tbl &lt;- bike_features_tbl %&gt;% \n  select(frame_material:gender, `Rear Derailleur`, `Shift Lever`) %&gt;% \n  mutate(\n    `shimano dura-ace`        = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano dura-ace \") %&gt;% as.numeric(),\n    `shimano ultegra`         = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano ultegra \") %&gt;% as.numeric(),\n    `shimano 105`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano 105 \") %&gt;% as.numeric(),\n    `shimano tiagra`          = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano tiagra \") %&gt;% as.numeric(),\n    `Shimano sora`            = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano sora\") %&gt;% as.numeric(),\n    `shimano deore`           = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano deore(?! xt)\") %&gt;% as.numeric(),\n    `shimano slx`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano slx\") %&gt;% as.numeric(),\n    `shimano grx`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano grx\") %&gt;% as.numeric(),\n    `Shimano xt`              = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano deore xt |shimano xt \") %&gt;% as.numeric(),\n    `Shimano xtr`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano xtr\") %&gt;% as.numeric(),\n    `Shimano saint`           = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano saint\") %&gt;% as.numeric(),\n    `SRAM red`                = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram red\") %&gt;% as.numeric(),\n    `SRAM force`              = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram force\") %&gt;% as.numeric(),\n    `SRAM rival`              = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram rival\") %&gt;% as.numeric(),\n    `SRAM apex`               = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram apex\") %&gt;% as.numeric(),\n    `SRAM xx1`                = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram xx1\") %&gt;% as.numeric(),\n    `SRAM x01`                = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram x01|sram xo1\") %&gt;% as.numeric(),\n    `SRAM gx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram gx\") %&gt;% as.numeric(),\n    `SRAM nx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram nx\") %&gt;% as.numeric(),\n    `SRAM sx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram sx\") %&gt;% as.numeric(),\n    `SRAM sx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram sx\") %&gt;% as.numeric(),\n    `Campagnolo potenza`      = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"campagnolo potenza\") %&gt;% as.numeric(),\n    `Campagnolo super record` = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"campagnolo super record\") %&gt;% as.numeric(),\n    `shimano nexus`           = `Shift Lever`     %&gt;% str_to_lower() %&gt;% str_detect(\"shimano nexus\") %&gt;% as.numeric(),\n    `shimano alfine`          = `Shift Lever`     %&gt;% str_to_lower() %&gt;% str_detect(\"shimano alfine\") %&gt;% as.numeric()\n  ) %&gt;%  \n  select(-c(`Rear Derailleur`, `Shift Lever`)) %&gt;% \n  mutate_if(is.numeric, ~replace(., is.na(.), 0)) \nbike_features_tbl &lt;- bike_features_tbl %&gt;% \n  mutate(id = row_number()) %&gt;% \n  mutate(frame_material = factor(frame_material)) %&gt;%\n  select(id, everything()) \nbike_features_tbl %&gt;% distinct(category_2)\n\n\n  \n\n\nsplit_obj &lt;- rsample::initial_split(bike_features_tbl, prop   = 0.80, \n                                    strata = \"category_2\")\nsplit_obj %&gt;% training() %&gt;% distinct(category_2)\n\n\n  \n\n\nsplit_obj %&gt;% testing() %&gt;% distinct(category_2)\n\n\n  \n\n\ntrain_tbl &lt;- training(split_obj)\ntest_tbl  &lt;- testing(split_obj)\ntrain_data &lt;- train_tbl %&gt;% set_names(str_replace_all(names(train_tbl), \" |-\", \"_\"))\ntest_data  &lt;- test_tbl  %&gt;% set_names(str_replace_all(names(test_tbl),  \" |-\", \"_\"))\n# recipe\nbike_rec &lt;- \n  recipe(frame_material ~ ., data = train_data) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes()) %&gt;% \n  step_zv(all_predictors()) \nd &lt;- summary(bike_rec)\nlr_mod &lt;- \n  logistic_reg() %&gt;% \n  set_engine(\"glm\")\nlr_mod\n\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glm\n\nbike_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lr_mod) %&gt;% \n  add_recipe(bike_rec)\nbike_wflow\n\n#&gt; ══ Workflow ════════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: logistic_reg()\n#&gt; \n#&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n#&gt; 2 Recipe Steps\n#&gt; \n#&gt; • step_dummy()\n#&gt; • step_zv()\n#&gt; \n#&gt; ── Model ───────────────────────────────────────────────────────────────────────\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glm\n\nbike_fit &lt;- \n  bike_wflow %&gt;% \n  fit(data = train_data)\n\n#&gt; Warning: glm.fit: algorithm did not converge\n\n\n#&gt; Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nbike_fit\n\n#&gt; ══ Workflow [trained] ══════════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: logistic_reg()\n#&gt; \n#&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n#&gt; 2 Recipe Steps\n#&gt; \n#&gt; • step_dummy()\n#&gt; • step_zv()\n#&gt; \n#&gt; ── Model ───────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;                (Intercept)                          id  \n#&gt;                 -9.990e+07                  -5.364e+04  \n#&gt;                     weight                       price  \n#&gt;                  2.379e+06                   2.187e+03  \n#&gt;           shimano_dura_ace             shimano_ultegra  \n#&gt;                 -5.954e+21                  -5.954e+21  \n#&gt;                shimano_105              shimano_tiagra  \n#&gt;                 -5.954e+21                  -5.954e+21  \n#&gt;              shimano_deore                 shimano_slx  \n#&gt;                 -5.181e+07                  -4.494e+15  \n#&gt;                shimano_grx                  Shimano_xt  \n#&gt;                 -5.954e+21                  -1.160e+09  \n#&gt;              Shimano_saint                    SRAM_red  \n#&gt;                  6.731e+08                  -5.954e+21  \n#&gt;                 SRAM_force                  SRAM_rival  \n#&gt;                 -5.954e+21                  -5.954e+21  \n#&gt;                  SRAM_apex                    SRAM_xx1  \n#&gt;                 -5.954e+21                  -1.777e+08  \n#&gt;                   SRAM_x01                     SRAM_gx  \n#&gt;                 -1.819e+08                  -9.665e+06  \n#&gt;                    SRAM_nx                     SRAM_sx  \n#&gt;                  9.109e+06                   5.544e+07  \n#&gt;         Campagnolo_potenza     Campagnolo_super_record  \n#&gt;                 -5.954e+21                  -5.954e+21  \n#&gt;              shimano_nexus              shimano_alfine  \n#&gt;                  1.063e+08                   1.820e+08  \n#&gt;          category_1_Gravel    category_1_Hybrid...City  \n#&gt;                  5.954e+21                  -1.768e+23  \n#&gt;        category_1_Mountain             category_1_Road  \n#&gt;                  5.854e+07                   5.954e+21  \n#&gt;        category_2_All.Road             category_2_City  \n#&gt;                  2.284e+07                   1.827e+23  \n#&gt;   category_2_Cross.Country       category_2_Cyclocross  \n#&gt;                  5.954e+21                   4.581e+10  \n#&gt;       category_2_Dirt.Jump         category_2_Downhill  \n#&gt;                 -2.050e+07                  -6.768e+08  \n#&gt;          category_2_E.City        category_2_E.Fitness  \n#&gt;                  4.675e+07                   4.112e+07  \n#&gt;        category_2_E.Gravel       category_2_E.Mountain  \n#&gt;                  5.954e+21                   4.845e+07  \n#&gt;          category_2_E.Road       category_2_E.Trekking  \n#&gt;                  5.954e+21                          NA  \n#&gt;       category_2_Endurance           category_2_Enduro  \n#&gt;                  1.495e+07                  -1.684e+07  \n#&gt;       category_2_Fat.Bikes             category_2_Race  \n#&gt;                  3.902e+07                   4.129e+15  \n#&gt; \n#&gt; ...\n#&gt; and 36 more lines.\n\nbike_fit %&gt;% \n  pull_workflow_fit() %&gt;% \n  tidy()\n\n#&gt; Warning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.\n#&gt; ℹ Please use `extract_fit_parsnip()` instead.\n\n\n\n  \n\n\nbike_pred &lt;- \n  predict(bike_fit, test_data, type=\"prob\") %&gt;% \n  bind_cols(test_data %&gt;% select(frame_material, category_2)) \n\n#&gt; Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\n#&gt; prediction from rank-deficient fit; attr(*, \"non-estim\") has doubtful cases\n\nbike_pred %&gt;% \n  roc_curve(truth = frame_material, .pred_aluminium) %&gt;% \n  autoplot()\n\n\n\n\n\n\nbike_pred %&gt;% \n  roc_curve(truth = frame_material, .pred_carbon) %&gt;% \n  autoplot()\n\n\n\n\n\n\nbike_pred\n\n\n  \n\n\nroc_al &lt;- bike_pred %&gt;% \n  roc_auc(truth = frame_material, .pred_aluminium)\nroc_al\n\n\n  \n\n\nroc_car &lt;- bike_pred %&gt;% \n  roc_auc(truth = frame_material, .pred_carbon)\nroc_car\n\n\n  \n\n\n# Evaluation\nmodel_01_linear_lm_simple &lt;- linear_reg(mode = \"regression\") %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(price ~ category_2 + frame_material, data = train_data)\nmodel_01_linear_lm_simple\n\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = price ~ category_2 + frame_material, data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;              (Intercept)        category_2All-Road            category_2City  \n#&gt;                   2329.5                    -522.7                   -1430.4  \n#&gt;  category_2Cross-Country      category_2Cyclocross       category_2Dirt Jump  \n#&gt;                    225.0                    -854.1                   -1063.9  \n#&gt;       category_2Downhill          category_2E-City       category_2E-Fitness  \n#&gt;                   2027.2                     698.8                     712.8  \n#&gt;       category_2E-Gravel      category_2E-Mountain          category_2E-Road  \n#&gt;                   1345.0                    1021.9                     589.5  \n#&gt;     category_2E-Trekking       category_2Endurance          category_2Enduro  \n#&gt;                   1097.5                    -486.6                     428.7  \n#&gt;      category_2Fat Bikes            category_2Race         category_2Touring  \n#&gt;                  -1460.0                     927.2                   -1176.5  \n#&gt;          category_2Trail  category_2Triathlon Bike      frame_materialcarbon  \n#&gt;                   -535.5                     267.0                    1514.5\n\ntest_data &lt;- test_data %&gt;% filter(category_2 != \"Fat Bikes\")\nyards &lt;- model_01_linear_lm_simple\nyards\n\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = price ~ category_2 + frame_material, data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;              (Intercept)        category_2All-Road            category_2City  \n#&gt;                   2329.5                    -522.7                   -1430.4  \n#&gt;  category_2Cross-Country      category_2Cyclocross       category_2Dirt Jump  \n#&gt;                    225.0                    -854.1                   -1063.9  \n#&gt;       category_2Downhill          category_2E-City       category_2E-Fitness  \n#&gt;                   2027.2                     698.8                     712.8  \n#&gt;       category_2E-Gravel      category_2E-Mountain          category_2E-Road  \n#&gt;                   1345.0                    1021.9                     589.5  \n#&gt;     category_2E-Trekking       category_2Endurance          category_2Enduro  \n#&gt;                   1097.5                    -486.6                     428.7  \n#&gt;      category_2Fat Bikes            category_2Race         category_2Touring  \n#&gt;                  -1460.0                     927.2                   -1176.5  \n#&gt;          category_2Trail  category_2Triathlon Bike      frame_materialcarbon  \n#&gt;                   -535.5                     267.0                    1514.5\n\ng1 &lt;- bike_features_tbl %&gt;% \n  mutate(category_2 = as.factor(category_2) %&gt;% \n           fct_reorder(price)) %&gt;% \n  \n  ggplot(aes(category_2, price)) +\n  geom_violin() +\n  geom_jitter(width = 0.1, alpha = 0.5, color = \"#FF00FF\") +\n  coord_flip() +\n  facet_wrap(~ frame_material) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  labs(\n    title = \"Unit Price for Each Model\",\n    y = \"\", x = \"Category 2\"\n  )\ng1\n\n#&gt; Warning: Groups with fewer than two datapoints have been dropped.\n#&gt; ℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\n#&gt; Warning: Groups with fewer than two datapoints have been dropped.\n#&gt; ℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n#&gt; Groups with fewer than two datapoints have been dropped.\n#&gt; ℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\n\n\n\n\n\nnew_race_alu &lt;- tibble(\n  model = \"Exceed AL SL new\",\n  category_2 = \"Race\",\n  frame_material = \"aluminium\",\n  shimano_dura_ace = 0,\n  shimano_ultegra = 0,\n  shimano_105 = 0,\n  shimano_tiagra = 0,\n  Shimano_sora = 0,\n  shimano_deore = 0,\n  shimano_slx = 0,\n  shimano_grx = 0,\n  Shimano_xt = 1,\n  Shimano_xtr = 0,\n  Shimano_saint = 0,\n  SRAM_red = 0,\n  SRAM_force = 0,\n  SRAM_rival = 0,\n  SRAM_apex = 0,\n  SRAM_xx1 = 0,\n  SRAM_x01 = 0,\n  SRAM_gx = 0,\n  SRAM_nx = 0,\n  SRAM_sx = 0,\n  Campagnolo_potenza = 0,\n  Campagnolo_super_record = 0,\n  shimano_nexus = 0,\n  shimano_alfine = 0\n) \nnew_race_alu\n\n\n  \n\n\npredict(model_01_linear_lm_simple, new_data = new_race_alu)\n\n\n  \n\n\nmodels_tbl &lt;- tibble(\n  model_id = str_c(\"Model 0\", 1:1),\n  model = list(\n    model_01_linear_lm_simple\n  )\n)\nmodels_tbl\n\n\n  \n\n\npredictions_new_race_alu_tbl &lt;- models_tbl %&gt;%\n  mutate(predictions = map(model, predict, new_data = new_race_alu)) %&gt;%\n  unnest(predictions) %&gt;%\n  mutate(category_2 = \"Race\") %&gt;%\n  left_join(new_race_alu, by = \"category_2\")\npredictions_new_race_alu_tbl\n\n\n  \n\n\ng2 &lt;- g1 +\n  geom_point(aes(y = .pred), color = \"red\", alpha = 0.5,\n             data = predictions_new_race_alu_tbl) +\n  ggrepel::geom_text_repel(aes(label = model_id, y = .pred),\n                           size = 3,\n                           data = predictions_new_race_alu_tbl)\ng2\n\n#&gt; Warning: Groups with fewer than two datapoints have been dropped.\n#&gt; ℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n#&gt; Groups with fewer than two datapoints have been dropped.\n#&gt; ℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n#&gt; Groups with fewer than two datapoints have been dropped.\n#&gt; ℹ Set `drop = FALSE` to consider such groups for position adjustment purposes."
  },
  {
    "objectID": "content/01_journal/01_Machine_learning_fundamentals.html",
    "href": "content/01_journal/01_Machine_learning_fundamentals.html",
    "title": "01 Machine Learning Fundamentals",
    "section": "",
    "text": "Load the absolute path to the data directory.\ndata_dir &lt;- params$data_dir"
  },
  {
    "objectID": "content/01_journal/01_Machine_learning_fundamentals.html#stock-prices-standardization",
    "href": "content/01_journal/01_Machine_learning_fundamentals.html#stock-prices-standardization",
    "title": "01 Machine Learning Fundamentals",
    "section": "2.1 Stock Prices Standardization",
    "text": "2.1 Stock Prices Standardization\nStock prices (adjusted stock price) are standardized by converting them into daily returns (percent change from previous day). This is done such that the stock prices are of the same magnitude and can thus be compared. Below is the sp 500 price table shown:\n\nsp_500_prices_tbl %&gt;% glimpse()\n\n#&gt; Rows: 1,225,765\n#&gt; Columns: 8\n#&gt; $ symbol   &lt;chr&gt; \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT…\n#&gt; $ date     &lt;date&gt; 2009-01-02, 2009-01-05, 2009-01-06, 2009-01-07, 2009-01-08, …\n#&gt; $ open     &lt;dbl&gt; 19.53, 20.20, 20.75, 20.19, 19.63, 20.17, 19.71, 19.52, 19.53…\n#&gt; $ high     &lt;dbl&gt; 20.40, 20.67, 21.00, 20.29, 20.19, 20.30, 19.79, 19.99, 19.68…\n#&gt; $ low      &lt;dbl&gt; 19.37, 20.06, 20.61, 19.48, 19.55, 19.41, 19.30, 19.52, 19.01…\n#&gt; $ close    &lt;dbl&gt; 20.33, 20.52, 20.76, 19.51, 20.12, 19.52, 19.47, 19.82, 19.09…\n#&gt; $ volume   &lt;dbl&gt; 50084000, 61475200, 58083400, 72709900, 70255400, 49815300, 5…\n#&gt; $ adjusted &lt;dbl&gt; 15.86624, 16.01451, 16.20183, 15.22628, 15.70234, 15.23408, 1…\n\n\n\nsp_500_daily_returns_tbl &lt;- sp_500_prices_tbl %&gt;%\n  select(symbol, date, adjusted) %&gt;%\n  filter(date &gt;= as.Date(\"2018-01-01\")) %&gt;%\n  group_by(symbol) %&gt;%\n  mutate(adjusted_lag = lag(adjusted)) %&gt;%\n  filter(!is.na(adjusted_lag)) %&gt;%\n  mutate(difference = adjusted - adjusted_lag) %&gt;%\n  mutate(pct_return = difference / adjusted_lag) %&gt;%\n  select(symbol, date, pct_return) %&gt;%\n  ungroup()\nprint(sp_500_daily_returns_tbl)\n\n#&gt; # A tibble: 141,340 × 3\n#&gt;    symbol date       pct_return\n#&gt;    &lt;chr&gt;  &lt;date&gt;          &lt;dbl&gt;\n#&gt;  1 MSFT   2018-01-03   0.00465 \n#&gt;  2 MSFT   2018-01-04   0.00880 \n#&gt;  3 MSFT   2018-01-05   0.0124  \n#&gt;  4 MSFT   2018-01-08   0.00102 \n#&gt;  5 MSFT   2018-01-09  -0.000680\n#&gt;  6 MSFT   2018-01-10  -0.00453 \n#&gt;  7 MSFT   2018-01-11   0.00296 \n#&gt;  8 MSFT   2018-01-12   0.0173  \n#&gt;  9 MSFT   2018-01-16  -0.0140  \n#&gt; 10 MSFT   2018-01-17   0.0203  \n#&gt; # ℹ 141,330 more rows"
  },
  {
    "objectID": "content/01_journal/01_Machine_learning_fundamentals.html#conversion-to-user-item-format",
    "href": "content/01_journal/01_Machine_learning_fundamentals.html#conversion-to-user-item-format",
    "title": "01 Machine Learning Fundamentals",
    "section": "2.2 Conversion to User-Item Format",
    "text": "2.2 Conversion to User-Item Format\nThe next step involves converting to a user-item format with the symbol in the first column and every other column the value of the daily returns (pct_return) for every stock at each date. The user in this case is the symbol (company), and the item in this case is the pct_return at each date.\nImporting the correct results first (just in case I was not able to complete the last step).\n\nsp_500_daily_returns_tbl &lt;- read_rds(\"C:/Users/risho/OneDrive/Desktop/internship_sparks/ss24-bdml-rishon1234/ML/data/Business Decisions with Machine Learning/sp_500_daily_returns_tbl.rds\")\nprint(sp_500_daily_returns_tbl)\n\n#&gt; # A tibble: 141,340 × 3\n#&gt;    symbol date       pct_return\n#&gt;    &lt;chr&gt;  &lt;date&gt;          &lt;dbl&gt;\n#&gt;  1 MSFT   2018-01-03   0.00465 \n#&gt;  2 MSFT   2018-01-04   0.00880 \n#&gt;  3 MSFT   2018-01-05   0.0124  \n#&gt;  4 MSFT   2018-01-08   0.00102 \n#&gt;  5 MSFT   2018-01-09  -0.000680\n#&gt;  6 MSFT   2018-01-10  -0.00453 \n#&gt;  7 MSFT   2018-01-11   0.00296 \n#&gt;  8 MSFT   2018-01-12   0.0173  \n#&gt;  9 MSFT   2018-01-16  -0.0140  \n#&gt; 10 MSFT   2018-01-17   0.0203  \n#&gt; # ℹ 141,330 more rows\n\n\nAnd the conversion follows with:\n\nstock_date_matrix_tbl &lt;- sp_500_daily_returns_tbl %&gt;%\n  spread(key = date, value = pct_return, fill = 0)\nstock_date_matrix_tbl |&gt; as_tibble() |&gt; print()\n\n#&gt; # A tibble: 502 × 283\n#&gt;    symbol `2018-01-03` `2018-01-04` `2018-01-05` `2018-01-08` `2018-01-09`\n#&gt;    &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n#&gt;  1 A          0.0254       -0.00750     0.0160        0.00215     0.0246  \n#&gt;  2 AAL       -0.0123        0.00630    -0.000380     -0.00988    -0.000959\n#&gt;  3 AAP        0.00905       0.0369      0.0106       -0.00704    -0.00808 \n#&gt;  4 AAPL      -0.000174      0.00465     0.0114       -0.00371    -0.000115\n#&gt;  5 ABBV       0.0156       -0.00570     0.0174       -0.0160      0.00754 \n#&gt;  6 ABC        0.00372      -0.00222     0.0121        0.0166      0.00640 \n#&gt;  7 ABMD       0.0173        0.0175      0.0154        0.0271      0.00943 \n#&gt;  8 ABT        0.00221      -0.00170     0.00289      -0.00288     0.00170 \n#&gt;  9 ACN        0.00462       0.0118      0.00825       0.00799     0.00333 \n#&gt; 10 ADBE       0.0188        0.0120      0.0116       -0.00162     0.00897 \n#&gt; # ℹ 492 more rows\n#&gt; # ℹ 277 more variables: `2018-01-10` &lt;dbl&gt;, `2018-01-11` &lt;dbl&gt;,\n#&gt; #   `2018-01-12` &lt;dbl&gt;, `2018-01-16` &lt;dbl&gt;, `2018-01-17` &lt;dbl&gt;,\n#&gt; #   `2018-01-18` &lt;dbl&gt;, `2018-01-19` &lt;dbl&gt;, `2018-01-22` &lt;dbl&gt;,\n#&gt; #   `2018-01-23` &lt;dbl&gt;, `2018-01-24` &lt;dbl&gt;, `2018-01-25` &lt;dbl&gt;,\n#&gt; #   `2018-01-26` &lt;dbl&gt;, `2018-01-29` &lt;dbl&gt;, `2018-01-30` &lt;dbl&gt;,\n#&gt; #   `2018-01-31` &lt;dbl&gt;, `2018-02-01` &lt;dbl&gt;, `2018-02-02` &lt;dbl&gt;, …"
  },
  {
    "objectID": "content/01_journal/01_Machine_learning_fundamentals.html#k-means-clustering",
    "href": "content/01_journal/01_Machine_learning_fundamentals.html#k-means-clustering",
    "title": "01 Machine Learning Fundamentals",
    "section": "2.3 K-Means Clustering",
    "text": "2.3 K-Means Clustering\nImporting the correct results first (just in case I was not able to complete the last step).\n\nstock_date_matrix_tbl &lt;- read_rds(\"C:/Users/risho/OneDrive/Desktop/internship_sparks/ss24-bdml-rishon1234/ML/data/Business Decisions with Machine Learning/stock_date_matrix_tbl.rds\")\nstock_date_matrix_tbl |&gt; as_tibble() |&gt; print()\n\n#&gt; # A tibble: 502 × 283\n#&gt;    symbol `2018-01-03` `2018-01-04` `2018-01-05` `2018-01-08` `2018-01-09`\n#&gt;    &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n#&gt;  1 A          0.0254       -0.00750     0.0160        0.00215     0.0246  \n#&gt;  2 AAL       -0.0123        0.00630    -0.000380     -0.00988    -0.000959\n#&gt;  3 AAP        0.00905       0.0369      0.0106       -0.00704    -0.00808 \n#&gt;  4 AAPL      -0.000174      0.00465     0.0114       -0.00371    -0.000115\n#&gt;  5 ABBV       0.0156       -0.00570     0.0174       -0.0160      0.00754 \n#&gt;  6 ABC        0.00372      -0.00222     0.0121        0.0166      0.00640 \n#&gt;  7 ABMD       0.0173        0.0175      0.0154        0.0271      0.00943 \n#&gt;  8 ABT        0.00221      -0.00170     0.00289      -0.00288     0.00170 \n#&gt;  9 ACN        0.00462       0.0118      0.00825       0.00799     0.00333 \n#&gt; 10 ADBE       0.0188        0.0120      0.0116       -0.00162     0.00897 \n#&gt; # ℹ 492 more rows\n#&gt; # ℹ 277 more variables: `2018-01-10` &lt;dbl&gt;, `2018-01-11` &lt;dbl&gt;,\n#&gt; #   `2018-01-12` &lt;dbl&gt;, `2018-01-16` &lt;dbl&gt;, `2018-01-17` &lt;dbl&gt;,\n#&gt; #   `2018-01-18` &lt;dbl&gt;, `2018-01-19` &lt;dbl&gt;, `2018-01-22` &lt;dbl&gt;,\n#&gt; #   `2018-01-23` &lt;dbl&gt;, `2018-01-24` &lt;dbl&gt;, `2018-01-25` &lt;dbl&gt;,\n#&gt; #   `2018-01-26` &lt;dbl&gt;, `2018-01-29` &lt;dbl&gt;, `2018-01-30` &lt;dbl&gt;,\n#&gt; #   `2018-01-31` &lt;dbl&gt;, `2018-02-01` &lt;dbl&gt;, `2018-02-02` &lt;dbl&gt;, …\n\n\nAnd then executing the KMeans operation:\n\n# Create kmeans_obj for 4 centers\nNUM_CENTERS &lt;- 4\nN_START = 20\n\nkmeans_obj &lt;- stock_date_matrix_tbl %&gt;%\n    select(-symbol) %&gt;%\n    kmeans(centers = NUM_CENTERS, nstart = N_START)\nprint(kmeans_obj$cluster)\n\n#&gt;   [1] 1 2 1 2 1 1 2 1 1 2 2 1 1 1 2 3 3 3 1 1 1 3 1 1 2 1 2 1 1 1 2 2 2 1 1 1 1\n#&gt;  [38] 3 2 2 2 1 1 1 4 4 1 1 1 3 1 3 2 3 2 1 3 1 1 1 1 1 1 1 1 1 1 4 1 1 1 1 1 1\n#&gt;  [75] 1 1 1 3 1 3 1 1 1 1 1 1 3 2 1 1 1 1 1 3 1 1 1 1 3 3 1 1 1 1 1 3 1 3 1 1 1\n#&gt; [112] 4 1 1 3 1 1 2 2 1 1 1 1 1 1 4 4 3 1 1 1 1 1 1 1 1 1 1 1 3 1 1 3 1 3 3 1 4\n#&gt; [149] 1 1 2 1 1 3 1 3 1 1 1 4 3 3 3 3 1 1 3 3 2 3 1 1 3 1 4 1 2 1 4 1 3 2 1 1 1\n#&gt; [186] 1 1 4 1 1 1 1 1 1 3 4 2 1 1 1 1 3 1 1 2 2 1 2 1 1 1 1 1 4 1 1 1 1 3 1 4 4\n#&gt; [223] 1 1 1 1 1 1 4 2 1 1 3 1 1 1 3 1 1 1 2 1 2 2 1 2 2 1 1 2 1 1 3 2 1 1 1 1 1\n#&gt; [260] 1 1 1 1 1 1 1 3 1 2 3 3 2 3 4 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 2 1 3 1\n#&gt; [297] 1 2 3 3 1 1 2 3 2 1 1 3 1 1 2 1 3 1 1 1 1 3 1 4 1 4 1 2 2 1 1 1 2 2 1 4 1\n#&gt; [334] 1 3 1 2 3 1 2 1 1 4 3 1 2 1 1 2 1 1 1 3 4 1 1 1 4 1 1 1 3 3 1 1 3 1 1 1 1\n#&gt; [371] 1 3 3 1 1 3 1 3 1 1 3 4 1 1 4 2 1 2 1 1 3 1 1 2 1 1 1 1 1 1 1 1 1 1 3 1 1\n#&gt; [408] 1 1 1 3 4 3 1 2 3 3 1 3 1 1 2 1 1 2 1 1 2 3 1 3 1 1 1 1 1 1 1 1 1 2 1 1 1\n#&gt; [445] 3 2 2 2 2 1 2 2 1 3 1 1 1 1 1 1 2 1 1 2 1 1 1 4 1 3 1 2 2 3 3 1 1 1 2 3 3\n#&gt; [482] 1 1 1 3 4 1 1 1 3 2 4 3 2 4 1 1 1 1 1 1 1\n\n\nAnd using glance() to get the tot.withinss.\n\nkmeans_obj %&gt;% glance() %&gt;% print()\n\n#&gt; # A tibble: 1 × 4\n#&gt;   totss tot.withinss betweenss  iter\n#&gt;   &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n#&gt; 1  33.6         29.2      4.40     4"
  },
  {
    "objectID": "content/01_journal/01_Machine_learning_fundamentals.html#finding-optimal-k",
    "href": "content/01_journal/01_Machine_learning_fundamentals.html#finding-optimal-k",
    "title": "01 Machine Learning Fundamentals",
    "section": "2.4 Finding Optimal K",
    "text": "2.4 Finding Optimal K\n\nkmeans_mapper &lt;- function(center = 3) {\n    stock_date_matrix_tbl %&gt;%\n        select(-symbol) %&gt;%\n        kmeans(centers = center, nstart = 20)\n}\n\n\n# Use purrr to map\nkmeans_mapped_tbl &lt;- tibble(centers = 1:30) %&gt;%\n    mutate(k_means = centers %&gt;% map(kmeans_mapper)) %&gt;%\n    mutate(glance  = k_means %&gt;% map(glance))\nprint(kmeans_mapped_tbl)\n\n#&gt; # A tibble: 30 × 3\n#&gt;    centers k_means  glance          \n#&gt;      &lt;int&gt; &lt;list&gt;   &lt;list&gt;          \n#&gt;  1       1 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  2       2 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  3       3 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  4       4 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  5       5 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  6       6 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  7       7 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  8       8 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  9       9 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt; 10      10 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt; # ℹ 20 more rows\n\n\n\n# Visualize Scree Plot\nkmeans_mapped_tbl %&gt;%\n    unnest(glance) %&gt;%\n    ggplot(aes(x = centers, y = tot.withinss)) +\n    geom_point() +\n    geom_line()"
  },
  {
    "objectID": "content/01_journal/01_Machine_learning_fundamentals.html#umap-application",
    "href": "content/01_journal/01_Machine_learning_fundamentals.html#umap-application",
    "title": "01 Machine Learning Fundamentals",
    "section": "2.5 UMAP Application",
    "text": "2.5 UMAP Application\n\nk_means_mapped_tbl &lt;- read_rds(\"C:/Users/risho/OneDrive/Desktop/internship_sparks/ss24-bdml-rishon1234/ML/data/Business Decisions with Machine Learning/k_means_mapped_tbl.rds\")\nprint(k_means_mapped_tbl)\n\n#&gt; # A tibble: 30 × 3\n#&gt;    centers k_means  glance          \n#&gt;      &lt;int&gt; &lt;list&gt;   &lt;list&gt;          \n#&gt;  1       1 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  2       2 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  3       3 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  4       4 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  5       5 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  6       6 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  7       7 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  8       8 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt;  9       9 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt; 10      10 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n#&gt; # ℹ 20 more rows\n\n\n\n# Apply UMAP\numap_results &lt;- stock_date_matrix_tbl %&gt;%\n  select(-symbol) %&gt;%\n  umap()\numap_results\n\n#&gt; umap embedding of 502 items in 2 dimensions\n#&gt; object components: layout, data, knn, config\n\n\n\n# Convert umap results to tibble with symbols\numap_results_tbl &lt;- umap_results$layout %&gt;%\n    as_tibble() %&gt;%\n    bind_cols(\n      stock_date_matrix_tbl %&gt;% select(symbol)\n  )\n\n#&gt; Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n#&gt; `.name_repair` is omitted as of tibble 2.0.0.\n#&gt; ℹ Using compatibility `.name_repair`.\n\nprint(umap_results_tbl)\n\n#&gt; # A tibble: 502 × 3\n#&gt;         V1      V2 symbol\n#&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; \n#&gt;  1 -1.56    0.148  A     \n#&gt;  2 -0.351   2.50   AAL   \n#&gt;  3 -0.0288 -1.18   AAP   \n#&gt;  4 -2.97   -0.641  AAPL  \n#&gt;  5  0.109   0.227  ABBV  \n#&gt;  6  0.550  -0.406  ABC   \n#&gt;  7 -2.92   -0.836  ABMD  \n#&gt;  8 -1.23   -0.0930 ABT   \n#&gt;  9 -1.66   -0.527  ACN   \n#&gt; 10 -2.98   -1.08   ADBE  \n#&gt; # ℹ 492 more rows\n\n\n\n# Visualize UMAP results\numap_results_tbl %&gt;%\n  ggplot(aes(x = V1, y = V2)) +\n  geom_point(alpha = 0.5) +\n  theme_tq() +\n  labs(title = \"UMAP Projection\")"
  },
  {
    "objectID": "content/01_journal/01_Machine_learning_fundamentals.html#combination-of-k-means-and-umap",
    "href": "content/01_journal/01_Machine_learning_fundamentals.html#combination-of-k-means-and-umap",
    "title": "01 Machine Learning Fundamentals",
    "section": "2.6 Combination of K-Means and UMAP",
    "text": "2.6 Combination of K-Means and UMAP\nNow the K-Means clusters and the UMAP 2D representation are being combined\nImporting the correct results first (just in case I was not able to complete the last step).\n\nk_means_mapped_tbl &lt;- read_rds(\"C:/Users/risho/OneDrive/Desktop/internship_sparks/ss24-bdml-rishon1234/ML/data/Business Decisions with Machine Learning/k_means_mapped_tbl.rds\")\numap_results_tbl   &lt;- read_rds(\"C:/Users/risho/OneDrive/Desktop/internship_sparks/ss24-bdml-rishon1234/ML/data/Business Decisions with Machine Learning/umap_results_tbl.rds\")\nprint(umap_results_tbl)\n\n#&gt; # A tibble: 502 × 3\n#&gt;         V1      V2 symbol\n#&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; \n#&gt;  1 -0.764   1.65   A     \n#&gt;  2 -2.70    0.455  AAL   \n#&gt;  3  0.739  -0.0320 AAP   \n#&gt;  4  0.0130  3.09   AAPL  \n#&gt;  5 -0.965  -0.0193 ABBV  \n#&gt;  6 -0.506  -0.659  ABC   \n#&gt;  7  0.436   3.10   ABMD  \n#&gt;  8 -0.262   1.35   ABT   \n#&gt;  9  0.0598  1.63   ACN   \n#&gt; 10  0.570   3.43   ADBE  \n#&gt; # ℹ 492 more rows\n\n\nNow, the first 10 KMeans items are to be selected as the ScreePlot flattens beyond this one.\n\n# Get the k_means_obj from the 10th center\nk_means_obj &lt;- k_means_mapped_tbl %&gt;%\n  pull(k_means) %&gt;%\n  pluck(10)\n\nNext, the clusters from the k_means_obj with the umap_results_tbl are being combined.\n\numap_kmeans_results_tbl &lt;- k_means_obj %&gt;%\n  augment(stock_date_matrix_tbl) %&gt;%\n  select(symbol, .cluster) %&gt;%\n  left_join(umap_results_tbl, by = \"symbol\") %&gt;%\n  left_join(sp_500_index_tbl %&gt;% select(symbol, company, sector), by = \"symbol\")\nprint(umap_kmeans_results_tbl)\n\n#&gt; # A tibble: 502 × 6\n#&gt;    symbol .cluster      V1      V2 company                       sector         \n#&gt;    &lt;chr&gt;  &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                         &lt;chr&gt;          \n#&gt;  1 A      7        -0.764   1.65   Agilent Technologies Inc.     Health Care    \n#&gt;  2 AAL    2        -2.70    0.455  American Airlines Group Inc.  Industrials    \n#&gt;  3 AAP    10        0.739  -0.0320 Advance Auto Parts Inc.       Consumer Discr…\n#&gt;  4 AAPL   9         0.0130  3.09   Apple Inc.                    Information Te…\n#&gt;  5 ABBV   7        -0.965  -0.0193 AbbVie Inc.                   Health Care    \n#&gt;  6 ABC    5        -0.506  -0.659  AmerisourceBergen Corporation Health Care    \n#&gt;  7 ABMD   9         0.436   3.10   ABIOMED Inc.                  Health Care    \n#&gt;  8 ABT    7        -0.262   1.35   Abbott Laboratories           Health Care    \n#&gt;  9 ACN    7         0.0598  1.63   Accenture Plc Class A         Information Te…\n#&gt; 10 ADBE   9         0.570   3.43   Adobe Inc.                    Information Te…\n#&gt; # ℹ 492 more rows\n\n\nAnd finally plotting the K-Means and UMAP results.\n\n# Visualize the combined K-Means and UMAP results\nlibrary(viridis)\n\n#&gt; Loading required package: viridisLite\n\numap_kmeans_results_tbl %&gt;%\n  ggplot(aes(x = V1, y = V2, color = .cluster)) +\n  geom_point(alpha = 0.5) +\n  scale_color_manual(values = viridis_pal()(10))"
  },
  {
    "objectID": "content/01_journal/03_Automated_ML_with_H2o.html",
    "href": "content/01_journal/03_Automated_ML_with_H2o.html",
    "title": "03 Automated Machine Learning with H20",
    "section": "",
    "text": "# Business case study Challenge 1\n\n# Libraries \nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(skimr)\nlibrary(GGally)\nlibrary(rsample)\n# Load Data data definitions\nemployee_attrition_tbl &lt;- read_csv(\"C:/Users/risho/OneDrive/Desktop/internship_sparks/ss24-bdml-rishon1234/ML/data/Business Decisions with Machine Learning/datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n\n#&gt; Rows: 1470 Columns: 35\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr  (9): Attrition, BusinessTravel, Department, EducationField, Gender, Job...\n#&gt; dbl (26): Age, DailyRate, DistanceFromHome, Education, EmployeeCount, Employ...\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npath_data_definitions &lt;- \"C:/Users/risho/OneDrive/Desktop/internship_sparks/ss24-bdml-rishon1234/ML/data/Business Decisions with Machine Learning/data_definitions.xlsx\"\ndefinitions_raw_tbl   &lt;- read_excel(path_data_definitions, sheet = 1, col_names = FALSE)\n\n#&gt; New names:\n#&gt; • `` -&gt; `...1`\n#&gt; • `` -&gt; `...2`\n\nemployee_attrition_tbl\n\n\n  \n\n\n# Business & Data Understanding: Department and Job Role\n# Data subset\ndept_job_role_tbl &lt;- employee_attrition_tbl %&gt;%\n  select(EmployeeNumber, Department, JobRole, PerformanceRating, Attrition)\ndept_job_role_tbl %&gt;%\n  group_by(Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(pct = n / sum(n))\n\n\n  \n\n\n# Attrition by department\ndept_job_role_tbl %&gt;%\n  # Block 1\n  group_by(Department, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  # Block 2: Caution: It's easy to inadvertently miss grouping when creating counts & percents within groups\n  group_by(Department) %&gt;%\n  mutate(pct = n / sum(n))\n\n#&gt; `summarise()` has grouped output by 'Department'. You can override using the\n#&gt; `.groups` argument.\n\n\n\n  \n\n\n# Attrition by job role\ndept_job_role_tbl %&gt;%\n  # Block 1\n  group_by(Department, JobRole, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  # Block 2\n  group_by(Department, JobRole) %&gt;%\n  mutate(pct = n / sum(n)) %&gt;%\n  ungroup() %&gt;%\n  # Block 3\n  filter(Attrition %in% \"Yes\")\n\n#&gt; `summarise()` has grouped output by 'Department', 'JobRole'. You can override\n#&gt; using the `.groups` argument.\n\n\n\n  \n\n\n# Develop KPI\ndept_job_role_tbl %&gt;%\n  # Block 1\n  group_by(Department, JobRole, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  # Block 2\n  group_by(Department, JobRole) %&gt;%\n  mutate(pct = n / sum(n)) %&gt;%\n  ungroup() %&gt;%\n  # Block 3\n  filter(Attrition %in% \"Yes\") %&gt;%\n  arrange(desc(pct)) %&gt;%\n  mutate(\n    above_industry_avg = case_when(\n      pct &gt; 0.088 ~ \"Yes\",\n      TRUE ~ \"No\"\n    )\n  )\n\n#&gt; `summarise()` has grouped output by 'Department', 'JobRole'. You can override\n#&gt; using the `.groups` argument.\n\n\n\n  \n\n\n# Function to calculate attrition cost\ncalculate_attrition_cost &lt;- function(\n    # Employee\n  n                    = 1,\n  salary               = 80000,\n  # Direct Costs\n  separation_cost      = 500,\n  vacancy_cost         = 10000,\n  acquisition_cost     = 4900,\n  placement_cost       = 3500,\n  # Productivity Costs\n  net_revenue_per_employee = 250000,\n  workdays_per_year        = 240,\n  workdays_position_open   = 40,\n  workdays_onboarding      = 60,\n  onboarding_efficiency    = 0.50\n) {\n  # Direct Costs\n  direct_cost &lt;- sum(separation_cost, vacancy_cost, acquisition_cost, placement_cost)\n  # Lost Productivity Costs\n  productivity_cost &lt;- net_revenue_per_employee / workdays_per_year *\n    (workdays_position_open + workdays_onboarding * onboarding_efficiency)\n  # Savings of Salary & Benefits (Cost Reduction)\n  salary_benefit_reduction &lt;- salary / workdays_per_year * workdays_position_open\n  # Estimated Turnover Per Employee\n  cost_per_employee &lt;- direct_cost + productivity_cost - salary_benefit_reduction\n  # Total Cost of Employee Turnover\n  total_cost &lt;- n * cost_per_employee\n  return(total_cost)\n}\ncalculate_attrition_cost()\n\n#&gt; [1] 78483.33\n\ncalculate_attrition_cost(200)\n\n#&gt; [1] 15696667\n\n# Use this\n# Function to convert counts to percentages. \ncount_to_pct &lt;- function(data, ..., col = n) {\n  # capture the dots\n  grouping_vars_expr &lt;- quos(...)\n  col_expr &lt;- enquo(col)\n  ret &lt;- data %&gt;%\n    group_by(!!! grouping_vars_expr) %&gt;%\n    mutate(pct = (!! col_expr) / sum(!! col_expr)) %&gt;%\n    ungroup()\n  return(ret)\n}\n# This is way shorter and more flexibel\ndept_job_role_tbl %&gt;%\n  count(JobRole, Attrition) %&gt;%\n  count_to_pct(JobRole)\n\n\n  \n\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition) %&gt;%\n  count_to_pct(Department, JobRole)  \n\n\n  \n\n\nassess_attrition &lt;- function(data, attrition_col, attrition_value, baseline_pct) {\n  attrition_col_expr &lt;- enquo(attrition_col)\n  data %&gt;%\n    \n    # Use parenthesis () to give tidy eval evaluation priority\n    filter((!! attrition_col_expr) %in% attrition_value) %&gt;%\n    arrange(desc(pct)) %&gt;%\n    mutate(\n      # Function inputs in numeric format (e.g. baseline_pct = 0.088 don't require tidy eval)\n      above_industry_avg = case_when(\n        pct &gt; baseline_pct ~ \"Yes\",\n        TRUE ~ \"No\"\n      )\n    )\n}\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition) %&gt;%\n  count_to_pct(Department, JobRole) %&gt;%\n  assess_attrition(Attrition, attrition_value = \"Yes\", baseline_pct = 0.088) %&gt;%\n  mutate(\n    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)\n  )\n\n\n  \n\n\ndept_job_role_tbl %&gt;%\n  group_by(Department, JobRole, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department, JobRole) %&gt;%\n  mutate(pct = n / sum(n)) %&gt;%\n  ungroup() %&gt;%\n  filter(Attrition %in% \"Yes\") %&gt;%\n  arrange(desc(pct)) %&gt;%\n  mutate(\n    above_industry_avg = case_when(\n      pct &gt; 0.088 ~ \"Yes\",\n      TRUE ~ \"No\"\n    )\n  ) %&gt;%\n  mutate(\n    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)\n  )\n\n#&gt; `summarise()` has grouped output by 'Department', 'JobRole'. You can override\n#&gt; using the `.groups` argument.\n\n\n\n  \n\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition) %&gt;%\n  count_to_pct(Department, JobRole) %&gt;%\n  assess_attrition(Attrition, attrition_value = \"Yes\", baseline_pct = 0.088) %&gt;%\n  mutate(\n    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)\n  ) %&gt;%\n  # Data Manipulation\n  mutate(name = str_c(Department, JobRole, sep = \": \") %&gt;% as_factor()) %&gt;%\n  # Check levels\n  # pull(name) %&gt;%\n  # levels()\n  mutate(name      = fct_reorder(name, cost_of_attrition)) %&gt;%\n  mutate(cost_text = str_c(\"$\", format(cost_of_attrition / 1e6, digits = 2),\n                           \"M\", sep = \"\")) %&gt;%\n  #Plotting\n  ggplot(aes(cost_of_attrition, y = name)) +\n  geom_segment(aes(xend = 0, yend = name),    color = \"#2dc6d6\") +\n  geom_point(  aes(size = cost_of_attrition), color = \"#2dc6d6\") +\n  scale_x_continuous(labels = scales::dollar) +\n  geom_label(aes(label = cost_text, size = cost_of_attrition),\n             hjust = \"inward\", color = \"#2dc6d6\") +\n  scale_size(range = c(3, 5)) +\n  labs(title = \"Estimated cost of Attrition: By Dept and Job Role\",\n       y = \"\",\n       x = \"Cost of attrition\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n# Function to plot attrition\nplot_attrition &lt;- function(data, \n                           ..., \n                           .value,\n                           fct_reorder = TRUE,\n                           fct_rev     = FALSE,\n                           include_lbl = TRUE,\n                           color       = \"#2dc6d6\",\n                           units       = c(\"0\", \"K\", \"M\")) {\n  ### Inputs\n  group_vars_expr   &lt;- quos(...)\n  \n  # If the user does not supply anything, \n  # this takes the first column of the supplied data\n  if (length(group_vars_expr) == 0) {\n    group_vars_expr &lt;- quos(rlang::sym(colnames(data)[[1]]))\n  }\n  value_expr &lt;- enquo(.value)\n  units_val  &lt;- switch(units[[1]],\n                       \"M\" = 1e6,\n                       \"K\" = 1e3,\n                       \"0\" = 1)\n  if (units[[1]] == \"0\") units &lt;- \"\"\n  # Data Manipulation\n  # This is a so called Function Factory (a function that produces a function)\n  usd &lt;- scales::dollar_format(prefix = \"$\", largest_with_cents = 1e3)\n  # Create the axis labels and values for the plot\n  data_manipulated &lt;- data %&gt;%\n    mutate(name = str_c(!!! group_vars_expr, sep = \": \") %&gt;% as_factor()) %&gt;%\n    mutate(value_text = str_c(usd(!! value_expr / units_val),\n                              units[[1]], sep = \"\"))\n  \n  # Order the labels on the y-axis according to the input\n  if (fct_reorder) {\n    data_manipulated &lt;- data_manipulated %&gt;%\n      mutate(name = forcats::fct_reorder(name, !! value_expr)) %&gt;%\n      arrange(name)\n  }\n  if (fct_rev) {\n    data_manipulated &lt;- data_manipulated %&gt;%\n      mutate(name = forcats::fct_rev(name)) %&gt;%\n      arrange(name)\n  }\n  # Visualization\n  g &lt;- data_manipulated %&gt;%\n    # \"name\" is a column name generated by our function internally as part of the data manipulation task\n    ggplot(aes(x = (!! value_expr), y = name)) +\n    geom_segment(aes(xend = 0, yend = name), color = color) +\n    geom_point(aes(size = !! value_expr), color = color) +\n    scale_x_continuous(labels = scales::dollar) +\n    scale_size(range = c(3, 5)) +\n    theme(legend.position = \"none\")\n  # Plot labels if TRUE\n  if (include_lbl) {\n    g &lt;- g +\n      geom_label(aes(label = value_text, size = !! value_expr),\n                 hjust = \"inward\", color = color)\n  }\n  return(g)\n}\ndept_job_role_tbl %&gt;%\n  # Select columnns\n  count(Department, JobRole, Attrition) %&gt;%\n  count_to_pct(Department, JobRole) %&gt;%\n  \n  assess_attrition(Attrition, attrition_value = \"Yes\", baseline_pct = 0.088) %&gt;%\n  mutate(\n    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)\n  ) %&gt;%\n  # Select columnns\n  plot_attrition(Department, JobRole, .value = cost_of_attrition,\n                 units = \"M\") +\n  labs(\n    title = \"Estimated Cost of Attrition by Job Role\",\n    x = \"Cost of Attrition\",\n    subtitle = \"Looks like Sales Executive and Labaratory Technician are the biggest drivers of cost\"\n  )\n\n\n\n\n\n\n# Step 1: Data Summarization -----\nskim(employee_attrition_tbl)\n\n\nData summary\n\n\nName\nemployee_attrition_tbl\n\n\nNumber of rows\n1470\n\n\nNumber of columns\n35\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n9\n\n\nnumeric\n26\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nAttrition\n0\n1\n2\n3\n0\n2\n0\n\n\nBusinessTravel\n0\n1\n10\n17\n0\n3\n0\n\n\nDepartment\n0\n1\n5\n22\n0\n3\n0\n\n\nEducationField\n0\n1\n5\n16\n0\n6\n0\n\n\nGender\n0\n1\n4\n6\n0\n2\n0\n\n\nJobRole\n0\n1\n7\n25\n0\n9\n0\n\n\nMaritalStatus\n0\n1\n6\n8\n0\n3\n0\n\n\nOver18\n0\n1\n1\n1\n0\n1\n0\n\n\nOverTime\n0\n1\n2\n3\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nAge\n0\n1\n36.92\n9.14\n18\n30.00\n36.0\n43.00\n60\n▂▇▇▃▂\n\n\nDailyRate\n0\n1\n802.49\n403.51\n102\n465.00\n802.0\n1157.00\n1499\n▇▇▇▇▇\n\n\nDistanceFromHome\n0\n1\n9.19\n8.11\n1\n2.00\n7.0\n14.00\n29\n▇▅▂▂▂\n\n\nEducation\n0\n1\n2.91\n1.02\n1\n2.00\n3.0\n4.00\n5\n▂▃▇▆▁\n\n\nEmployeeCount\n0\n1\n1.00\n0.00\n1\n1.00\n1.0\n1.00\n1\n▁▁▇▁▁\n\n\nEmployeeNumber\n0\n1\n1024.87\n602.02\n1\n491.25\n1020.5\n1555.75\n2068\n▇▇▇▇▇\n\n\nEnvironmentSatisfaction\n0\n1\n2.72\n1.09\n1\n2.00\n3.0\n4.00\n4\n▅▅▁▇▇\n\n\nHourlyRate\n0\n1\n65.89\n20.33\n30\n48.00\n66.0\n83.75\n100\n▇▇▇▇▇\n\n\nJobInvolvement\n0\n1\n2.73\n0.71\n1\n2.00\n3.0\n3.00\n4\n▁▃▁▇▁\n\n\nJobLevel\n0\n1\n2.06\n1.11\n1\n1.00\n2.0\n3.00\n5\n▇▇▃▂▁\n\n\nJobSatisfaction\n0\n1\n2.73\n1.10\n1\n2.00\n3.0\n4.00\n4\n▅▅▁▇▇\n\n\nMonthlyIncome\n0\n1\n6502.93\n4707.96\n1009\n2911.00\n4919.0\n8379.00\n19999\n▇▅▂▁▂\n\n\nMonthlyRate\n0\n1\n14313.10\n7117.79\n2094\n8047.00\n14235.5\n20461.50\n26999\n▇▇▇▇▇\n\n\nNumCompaniesWorked\n0\n1\n2.69\n2.50\n0\n1.00\n2.0\n4.00\n9\n▇▃▂▂▁\n\n\nPercentSalaryHike\n0\n1\n15.21\n3.66\n11\n12.00\n14.0\n18.00\n25\n▇▅▃▂▁\n\n\nPerformanceRating\n0\n1\n3.15\n0.36\n3\n3.00\n3.0\n3.00\n4\n▇▁▁▁▂\n\n\nRelationshipSatisfaction\n0\n1\n2.71\n1.08\n1\n2.00\n3.0\n4.00\n4\n▅▅▁▇▇\n\n\nStandardHours\n0\n1\n80.00\n0.00\n80\n80.00\n80.0\n80.00\n80\n▁▁▇▁▁\n\n\nStockOptionLevel\n0\n1\n0.79\n0.85\n0\n0.00\n1.0\n1.00\n3\n▇▇▁▂▁\n\n\nTotalWorkingYears\n0\n1\n11.28\n7.78\n0\n6.00\n10.0\n15.00\n40\n▇▇▂▁▁\n\n\nTrainingTimesLastYear\n0\n1\n2.80\n1.29\n0\n2.00\n3.0\n3.00\n6\n▂▇▇▂▃\n\n\nWorkLifeBalance\n0\n1\n2.76\n0.71\n1\n2.00\n3.0\n3.00\n4\n▁▃▁▇▂\n\n\nYearsAtCompany\n0\n1\n7.01\n6.13\n0\n3.00\n5.0\n9.00\n40\n▇▂▁▁▁\n\n\nYearsInCurrentRole\n0\n1\n4.23\n3.62\n0\n2.00\n3.0\n7.00\n18\n▇▃▂▁▁\n\n\nYearsSinceLastPromotion\n0\n1\n2.19\n3.22\n0\n0.00\n1.0\n3.00\n15\n▇▁▁▁▁\n\n\nYearsWithCurrManager\n0\n1\n4.12\n3.57\n0\n2.00\n3.0\n7.00\n17\n▇▂▅▁▁\n\n\n\n\n# Character Data Type\nemployee_attrition_tbl %&gt;%\n  select_if(is.character) %&gt;%\n  glimpse()\n\n#&gt; Rows: 1,470\n#&gt; Columns: 9\n#&gt; $ Attrition      &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n#&gt; $ BusinessTravel &lt;chr&gt; \"Travel_Rarely\", \"Travel_Frequently\", \"Travel_Rarely\", …\n#&gt; $ Department     &lt;chr&gt; \"Sales\", \"Research & Development\", \"Research & Developm…\n#&gt; $ EducationField &lt;chr&gt; \"Life Sciences\", \"Life Sciences\", \"Other\", \"Life Scienc…\n#&gt; $ Gender         &lt;chr&gt; \"Female\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Fe…\n#&gt; $ JobRole        &lt;chr&gt; \"Sales Executive\", \"Research Scientist\", \"Laboratory Te…\n#&gt; $ MaritalStatus  &lt;chr&gt; \"Single\", \"Married\", \"Single\", \"Married\", \"Married\", \"S…\n#&gt; $ Over18         &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", …\n#&gt; $ OverTime       &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No…\n\n# Get \"levels\"\nemployee_attrition_tbl %&gt;%\n  select_if(is.character) %&gt;%\n  map(unique)\n\n#&gt; $Attrition\n#&gt; [1] \"Yes\" \"No\" \n#&gt; \n#&gt; $BusinessTravel\n#&gt; [1] \"Travel_Rarely\"     \"Travel_Frequently\" \"Non-Travel\"       \n#&gt; \n#&gt; $Department\n#&gt; [1] \"Sales\"                  \"Research & Development\" \"Human Resources\"       \n#&gt; \n#&gt; $EducationField\n#&gt; [1] \"Life Sciences\"    \"Other\"            \"Medical\"          \"Marketing\"       \n#&gt; [5] \"Technical Degree\" \"Human Resources\" \n#&gt; \n#&gt; $Gender\n#&gt; [1] \"Female\" \"Male\"  \n#&gt; \n#&gt; $JobRole\n#&gt; [1] \"Sales Executive\"           \"Research Scientist\"       \n#&gt; [3] \"Laboratory Technician\"     \"Manufacturing Director\"   \n#&gt; [5] \"Healthcare Representative\" \"Manager\"                  \n#&gt; [7] \"Sales Representative\"      \"Research Director\"        \n#&gt; [9] \"Human Resources\"          \n#&gt; \n#&gt; $MaritalStatus\n#&gt; [1] \"Single\"   \"Married\"  \"Divorced\"\n#&gt; \n#&gt; $Over18\n#&gt; [1] \"Y\"\n#&gt; \n#&gt; $OverTime\n#&gt; [1] \"Yes\" \"No\"\n\n# Proportions    \nemployee_attrition_tbl %&gt;%\n  select_if(is.character) %&gt;%\n  map(~ table(.) %&gt;% prop.table())\n\n#&gt; $Attrition\n#&gt; .\n#&gt;        No       Yes \n#&gt; 0.8387755 0.1612245 \n#&gt; \n#&gt; $BusinessTravel\n#&gt; .\n#&gt;        Non-Travel Travel_Frequently     Travel_Rarely \n#&gt;         0.1020408         0.1884354         0.7095238 \n#&gt; \n#&gt; $Department\n#&gt; .\n#&gt;        Human Resources Research & Development                  Sales \n#&gt;             0.04285714             0.65374150             0.30340136 \n#&gt; \n#&gt; $EducationField\n#&gt; .\n#&gt;  Human Resources    Life Sciences        Marketing          Medical \n#&gt;       0.01836735       0.41224490       0.10816327       0.31564626 \n#&gt;            Other Technical Degree \n#&gt;       0.05578231       0.08979592 \n#&gt; \n#&gt; $Gender\n#&gt; .\n#&gt; Female   Male \n#&gt;    0.4    0.6 \n#&gt; \n#&gt; $JobRole\n#&gt; .\n#&gt; Healthcare Representative           Human Resources     Laboratory Technician \n#&gt;                0.08911565                0.03537415                0.17619048 \n#&gt;                   Manager    Manufacturing Director         Research Director \n#&gt;                0.06938776                0.09863946                0.05442177 \n#&gt;        Research Scientist           Sales Executive      Sales Representative \n#&gt;                0.19863946                0.22176871                0.05646259 \n#&gt; \n#&gt; $MaritalStatus\n#&gt; .\n#&gt;  Divorced   Married    Single \n#&gt; 0.2224490 0.4578231 0.3197279 \n#&gt; \n#&gt; $Over18\n#&gt; .\n#&gt; Y \n#&gt; 1 \n#&gt; \n#&gt; $OverTime\n#&gt; .\n#&gt;        No       Yes \n#&gt; 0.7170068 0.2829932\n\n# Numeric Data\nemployee_attrition_tbl %&gt;%\n  select_if(is.numeric) %&gt;%\n  map(~ unique(.) %&gt;% length())\n\n#&gt; $Age\n#&gt; [1] 43\n#&gt; \n#&gt; $DailyRate\n#&gt; [1] 886\n#&gt; \n#&gt; $DistanceFromHome\n#&gt; [1] 29\n#&gt; \n#&gt; $Education\n#&gt; [1] 5\n#&gt; \n#&gt; $EmployeeCount\n#&gt; [1] 1\n#&gt; \n#&gt; $EmployeeNumber\n#&gt; [1] 1470\n#&gt; \n#&gt; $EnvironmentSatisfaction\n#&gt; [1] 4\n#&gt; \n#&gt; $HourlyRate\n#&gt; [1] 71\n#&gt; \n#&gt; $JobInvolvement\n#&gt; [1] 4\n#&gt; \n#&gt; $JobLevel\n#&gt; [1] 5\n#&gt; \n#&gt; $JobSatisfaction\n#&gt; [1] 4\n#&gt; \n#&gt; $MonthlyIncome\n#&gt; [1] 1349\n#&gt; \n#&gt; $MonthlyRate\n#&gt; [1] 1427\n#&gt; \n#&gt; $NumCompaniesWorked\n#&gt; [1] 10\n#&gt; \n#&gt; $PercentSalaryHike\n#&gt; [1] 15\n#&gt; \n#&gt; $PerformanceRating\n#&gt; [1] 2\n#&gt; \n#&gt; $RelationshipSatisfaction\n#&gt; [1] 4\n#&gt; \n#&gt; $StandardHours\n#&gt; [1] 1\n#&gt; \n#&gt; $StockOptionLevel\n#&gt; [1] 4\n#&gt; \n#&gt; $TotalWorkingYears\n#&gt; [1] 40\n#&gt; \n#&gt; $TrainingTimesLastYear\n#&gt; [1] 7\n#&gt; \n#&gt; $WorkLifeBalance\n#&gt; [1] 4\n#&gt; \n#&gt; $YearsAtCompany\n#&gt; [1] 37\n#&gt; \n#&gt; $YearsInCurrentRole\n#&gt; [1] 19\n#&gt; \n#&gt; $YearsSinceLastPromotion\n#&gt; [1] 16\n#&gt; \n#&gt; $YearsWithCurrManager\n#&gt; [1] 18\n\nemployee_attrition_tbl %&gt;%\n  select_if(is.numeric) %&gt;%\n  map_df(~ unique(.) %&gt;% length()) %&gt;%\n  # Select all columns\n  pivot_longer(everything()) %&gt;%\n  arrange(value) %&gt;%\n  filter(value &lt;= 10)\n\n\n  \n\n\n# Step 2: Data Visualization ----\nemployee_attrition_tbl %&gt;%\n  select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %&gt;%\n  ggpairs() \n\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %&gt;%\n  ggpairs(aes(color = Attrition), lower = \"blank\", legend = 1,\n          diag  = list(continuous = wrap(\"densityDiag\", alpha = 0.5))) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nplot_ggpairs &lt;- function(data, color = NULL, density_alpha = 0.5) {\n  \n  color_expr &lt;- enquo(color)\n  \n  if (rlang::quo_is_null(color_expr)) {\n    \n    g &lt;- data %&gt;%\n      ggpairs(lower = \"blank\") \n    \n  } else {\n    \n    color_name &lt;- quo_name(color_expr)\n    \n    g &lt;- data %&gt;%\n      ggpairs(mapping = aes_string(color = color_name), \n              lower = \"blank\", legend = 1,\n              diag = list(continuous = wrap(\"densityDiag\", \n                                            alpha = density_alpha))) +\n      theme(legend.position = \"bottom\",\n            text = element_text(size=8),\n            axis.text = element_text(size = 10),\n            axis.title = element_text(size = 10))\n  }\n  \n  return(g)\n  \n}\n\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition,  contains(\"Training\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`.\n#&gt; ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\n\n\n\n\n\n\n\n1 Challenge 1\nUse your learning from descriptive features and plot_ggpairs() to further investigate the features. Run the functions above according to the features needed. Answer the following questions. Most of the time, you will only need the images from diagonal.\n\n\nWhat can you deduce about the interaction between Monthly Income and Attrition? c\n\n\n\nThose that are leaving the company have a higher Monthly Income\nThat those are staying have a lower Monthly Income\nThose that are leaving have a lower Monthly Income\nIt’s difficult to deduce anything based on the visualization\n\n\n\nWhat can you deduce about the interaction between Percent Salary Hike and Attrition? d\n\n\n\nThose that are leaving the company have a higher Percent Salary Hike\nThose that are staying have a lower Percent Salary Hike\nThose that are leaving have lower Percent Salary Hike\nIt’s difficult to deduce anything based on the visualization\n\n\n\nWhat can you deduce about the interaction between Stock Option Level and Attrition? b\n\n\n\nThose that are leaving the company have a higher stock option level\nThose that are staying have a higher stock option level\nIt’s difficult to deduce anything based on the visualization\n\n\n\nWhat can you deduce about the interaction between Environment Satisfaction and Attrition? a\n\n\n\nA higher proportion of those leaving have a low environment satisfaction level\nA higher proportion of those leaving have a high environment satisfaction level\nIt’s difficult to deduce anything based on the visualization\n\n\n\nWhat can you deduce about the interaction between Work Life Balance and Attrition b\n\n\n\nThose that are leaving have higher density of 2’s and 3’s\nThose that are staying have a higher density of 2’s and 3’s\nThose that are staying have a lower density of 2’s and 3’s\nIt’s difficult to deduce anything based on the visualization\n\n\n\nWhat Can you deduce about the interaction between Job Involvement and Attrition? a\n\n\n\nThose that are leaving have a lower density of 3’s and 4’s\nThose that are leaving have a lower density of 1’s and 2’s\nThose that are staying have a lower density of 2’s and 3’s\nIt’s difficult to deduce anything based on the visualization\n\n\n\nWhat can you deduce about the interaction between Over Time and Attrition? a\n\n\n\nThe proportion of those leaving that are working Over Time are high compared to those that are not leaving\nThe proportion of those staying that are working Over Time are high compared to those that are not staying\n\n\n\nWhat can you deduce about the interaction between Training Times Last Year and Attrition b\n\n\n\nPeople that leave tend to have more annual trainings\nPeople that leave tend to have less annual trainings\nIt’s difficult to deduce anything based on the visualization\n\n\n\nWhat can you deduce about the interaction between Years At Company and Attrition b\n\n\n\nPeople that leave tend to have more working years at the company\nPeople that leave tend to have less working years at the company\nIt’s difficult to deduce anything based on the visualization\n\n\n\nWhat can you deduce about the interaction between Years Since Last Promotion and Attrition? c\n\n\n\nThose that are leaving have more years since last promotion than those that are staying\nThose that are leaving have fewer years since last promotion than those that are staying\nIt’s difficult to deduce anything based on the visualization\n\n\n#Challenge 2\n## Load the training & test dataset\nlibrary(tidyverse)\n# Modeling\nlibrary(parsnip)\n# Pre-processing & Sampling\nlibrary(recipes)\nlibrary(rsample)\n# Modeling Error Metrics\nlibrary(yardstick)\nlibrary(workflows)\nlibrary(tune)\n\nproduct_data &lt;- read_csv(\"C:/Users/risho/OneDrive/Desktop/internship_sparks/ss24-bdml-rishon1234/ML/data/Business Decisions with Machine Learning/product_backorders.csv\")\n\n#&gt; Rows: 19053 Columns: 23\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr  (7): potential_issue, deck_risk, oe_constraint, ppap_risk, stop_auto_bu...\n#&gt; dbl (16): sku, national_inv, lead_time, in_transit_qty, forecast_3_month, fo...\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nproduct_data2 &lt;- product_data %&gt;% \n  mutate(\n    product_backorder = went_on_backorder %&gt;% str_to_lower() %&gt;% str_detect(\"yes\") %&gt;% as.numeric()\n  ) %&gt;% \n  select(-c(went_on_backorder))\nglimpse(product_data)\n\n#&gt; Rows: 19,053\n#&gt; Columns: 23\n#&gt; $ sku               &lt;dbl&gt; 1113121, 1113268, 1113874, 1114222, 1114823, 1115453…\n#&gt; $ national_inv      &lt;dbl&gt; 0, 0, 20, 0, 0, 55, -34, 4, 2, -7, 1, 2, 0, 0, 0, 0,…\n#&gt; $ lead_time         &lt;dbl&gt; 8, 8, 2, 8, 12, 8, 8, 9, 8, 8, 8, 8, 12, 2, 12, 4, 2…\n#&gt; $ in_transit_qty    &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…\n#&gt; $ forecast_3_month  &lt;dbl&gt; 6, 2, 45, 9, 31, 216, 120, 43, 4, 56, 2, 5, 5, 54, 4…\n#&gt; $ forecast_6_month  &lt;dbl&gt; 6, 3, 99, 14, 31, 360, 240, 67, 6, 96, 4, 9, 6, 72, …\n#&gt; $ forecast_9_month  &lt;dbl&gt; 6, 4, 153, 21, 31, 492, 240, 115, 9, 112, 6, 13, 9, …\n#&gt; $ sales_1_month     &lt;dbl&gt; 0, 1, 16, 5, 7, 30, 83, 5, 1, 13, 0, 1, 0, 0, 1, 0, …\n#&gt; $ sales_3_month     &lt;dbl&gt; 4, 2, 42, 17, 15, 108, 122, 22, 5, 30, 2, 5, 4, 0, 3…\n#&gt; $ sales_6_month     &lt;dbl&gt; 9, 3, 80, 36, 33, 275, 144, 40, 6, 56, 3, 8, 5, 0, 4…\n#&gt; $ sales_9_month     &lt;dbl&gt; 12, 3, 111, 43, 47, 340, 165, 58, 9, 76, 4, 11, 6, 0…\n#&gt; $ min_bank          &lt;dbl&gt; 0, 0, 10, 0, 2, 51, 33, 4, 2, 0, 0, 0, 3, 4, 0, 0, 0…\n#&gt; $ potential_issue   &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n#&gt; $ pieces_past_due   &lt;dbl&gt; 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#&gt; $ perf_6_month_avg  &lt;dbl&gt; 0.90, 0.96, 0.81, 0.96, 0.98, 0.00, 1.00, 0.69, 1.00…\n#&gt; $ perf_12_month_avg &lt;dbl&gt; 0.89, 0.97, 0.88, 0.98, 0.98, 0.00, 0.97, 0.68, 0.95…\n#&gt; $ local_bo_qty      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 34, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ deck_risk         &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n#&gt; $ oe_constraint     &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n#&gt; $ ppap_risk         &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No…\n#&gt; $ stop_auto_buy     &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Ye…\n#&gt; $ rev_stop          &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n#&gt; $ went_on_backorder &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Ye…\n\nsplit_obj&lt;- initial_split(product_data2, prop = 0.75)\ntrain_tbl&lt;- training(split_obj)\ntest_tbl&lt;- testing(split_obj)\n\n## Specifiy the response and predictor variables\nrecipe_obj &lt;- recipe(product_backorder ~., data = train_tbl) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_dummy(all_nominal(),-all_outcomes()) %&gt;%\n  prep()\n\nsummary(recipe_obj)\n\n\n  \n\n\nglimpse(bake(recipe_obj,new_data = NULL))\n\n#&gt; Rows: 14,289\n#&gt; Columns: 23\n#&gt; $ sku                 &lt;dbl&gt; 2172734, 1757139, 2270243, 2042576, 1691946, 21399…\n#&gt; $ national_inv        &lt;dbl&gt; 2, 2, 5, 11, 57, 12, 4, 5, 2, 7, 3, 4, 15, 13, 308…\n#&gt; $ lead_time           &lt;dbl&gt; NA, 8, 2, 9, 8, 9, 2, 2, 8, 4, 8, 8, 8, 2, 8, 8, N…\n#&gt; $ in_transit_qty      &lt;dbl&gt; 0, 0, 9, 0, 6, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,…\n#&gt; $ forecast_3_month    &lt;dbl&gt; 0, 5, 27, 0, 80, 0, 0, 0, 0, 0, 5, 0, 0, 20, 0, 0,…\n#&gt; $ forecast_6_month    &lt;dbl&gt; 0, 9, 54, 0, 135, 0, 4, 0, 0, 0, 11, 0, 0, 45, 0, …\n#&gt; $ forecast_9_month    &lt;dbl&gt; 0, 13, 72, 0, 185, 0, 9, 0, 0, 0, 17, 0, 0, 65, 0,…\n#&gt; $ sales_1_month       &lt;dbl&gt; 0, 3, 15, 1, 15, 0, 0, 0, 0, 0, 2, 0, 0, 5, 11, 1,…\n#&gt; $ sales_3_month       &lt;dbl&gt; 0, 4, 32, 1, 89, 0, 5, 0, 0, 0, 7, 1, 0, 15, 16, 1…\n#&gt; $ sales_6_month       &lt;dbl&gt; 0, 9, 52, 1, 164, 0, 8, 0, 0, 0, 14, 1, 4, 27, 53,…\n#&gt; $ sales_9_month       &lt;dbl&gt; 0, 13, 77, 2, 277, 0, 11, 0, 0, 0, 22, 1, 5, 37, 1…\n#&gt; $ min_bank            &lt;dbl&gt; 0, 0, 7, 0, 48, 0, 0, 0, 0, 1, 0, 1, 2, 0, 23, 2, …\n#&gt; $ pieces_past_due     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ perf_6_month_avg    &lt;dbl&gt; -99.00, 1.00, 1.00, 0.86, 0.58, 0.99, 0.82, 0.88, …\n#&gt; $ perf_12_month_avg   &lt;dbl&gt; -99.00, 0.95, 0.94, 0.84, 0.57, 0.96, 0.81, 0.63, …\n#&gt; $ local_bo_qty        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ product_backorder   &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ potential_issue_Yes &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ deck_risk_Yes       &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,…\n#&gt; $ oe_constraint_Yes   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ ppap_risk_Yes       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ stop_auto_buy_Yes   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,…\n#&gt; $ rev_stop_Yes        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\nlibrary(h2o)\nh2o.init()\n\n#&gt; \n#&gt; H2O is not running yet, starting it now...\n#&gt; \n#&gt; Note:  In case of errors look at the following log files:\n#&gt;     C:\\Users\\risho\\AppData\\Local\\Temp\\RtmpInQ4LL\\file36207f74ca/h2o_risho_started_from_r.out\n#&gt;     C:\\Users\\risho\\AppData\\Local\\Temp\\RtmpInQ4LL\\file3620213f62c0/h2o_risho_started_from_r.err\n#&gt; \n#&gt; \n#&gt; Starting H2O JVM and connecting:  Connection successful!\n#&gt; \n#&gt; R is connected to the H2O cluster: \n#&gt;     H2O cluster uptime:         3 seconds 247 milliseconds \n#&gt;     H2O cluster timezone:       Europe/Berlin \n#&gt;     H2O data parsing timezone:  UTC \n#&gt;     H2O cluster version:        3.44.0.3 \n#&gt;     H2O cluster version age:    6 months and 4 days \n#&gt;     H2O cluster name:           H2O_started_from_R_risho_tbu647 \n#&gt;     H2O cluster total nodes:    1 \n#&gt;     H2O cluster total memory:   1.91 GB \n#&gt;     H2O cluster total cores:    12 \n#&gt;     H2O cluster allowed cores:  12 \n#&gt;     H2O cluster healthy:        TRUE \n#&gt;     H2O Connection ip:          localhost \n#&gt;     H2O Connection port:        54321 \n#&gt;     H2O Connection proxy:       NA \n#&gt;     H2O Internal Security:      FALSE \n#&gt;     R Version:                  R version 4.4.0 (2024-04-24 ucrt)\n\n\n#&gt; Warning in h2o.clusterInfo(): \n#&gt; Your H2O cluster version is (6 months and 4 days) old. There may be a newer version available.\n#&gt; Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n\nsplit_h2o &lt;- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.75), seed = 42)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntrain_h2o &lt;- split_h2o[[1]]\nvalid_h2o &lt;- split_h2o[[2]]\ntest_h2o  &lt;- as.h2o(test_tbl)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Set the target and predictors\ny &lt;- \"product_backorder\"\nx &lt;- setdiff(names(train_h2o), y)\n\nautoml_models_h2o &lt;- h2o.automl(\n  x = x,\n  y = y,\n  training_frame    = train_h2o,\n  validation_frame  = valid_h2o,\n  leaderboard_frame = test_h2o,\n  max_runtime_secs  = 120,\n  nfolds            = 5,\n  stopping_metric = \"mae\", stopping_rounds = 3,\n  stopping_tolerance = 1e-2\n)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   2%\n#&gt; 18:01:53.10: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n#&gt; 18:01:53.30: AutoML: XGBoost is not available; skipping it.\n#&gt; 18:01:53.86: _train param, Dropping bad and constant columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#&gt; 18:01:53.86: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n#&gt; 18:01:54.386: _train param, Dropping bad and constant columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#&gt; 18:01:54.386: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |=====                                                                 |   8%\n#&gt; 18:02:03.388: _train param, Dropping unused columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#&gt; 18:02:03.388: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |=======                                                               |  10%\n#&gt; 18:02:04.739: _train param, Dropping bad and constant columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#&gt; 18:02:04.739: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |=========                                                             |  12%\n#&gt; 18:02:07.938: _train param, Dropping bad and constant columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#&gt; 18:02:07.938: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |==========                                                            |  15%\n#&gt; 18:02:09.698: _train param, Dropping bad and constant columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#&gt; 18:02:09.698: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n#&gt; 18:02:11.264: _train param, Dropping bad and constant columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#&gt; 18:02:11.264: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |============                                                          |  17%\n#&gt; 18:02:12.880: _train param, Dropping unused columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#&gt; 18:02:12.880: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n#&gt; 18:02:13.484: _train param, Dropping unused columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#&gt; 18:02:13.484: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n#&gt; 18:02:14.44: _train param, Dropping bad and constant columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#&gt; 18:02:14.44: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |==============                                                        |  20%\n#&gt; 18:02:16.414: _train param, Dropping bad and constant columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#&gt; 18:02:16.414: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n#&gt; 18:02:17.888: _train param, Dropping bad and constant columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#&gt; 18:02:17.888: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |=================                                                     |  24%\n#&gt; 18:02:20.339: _train param, Dropping unused columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#&gt; 18:02:20.339: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n#&gt; 18:02:20.827: _train param, Dropping unused columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#&gt; 18:02:20.827: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |===============================================================       |  91%\n#&gt; 18:03:41.85: _train param, Dropping unused columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#&gt; 18:03:41.85: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n## View the leaderboard\nautoml_models_h2o@leaderboard \n\n#&gt;                                                  model_id      rmse        mse\n#&gt; 1    StackedEnsemble_AllModels_3_AutoML_1_20240624_180152 0.2227469 0.04961620\n#&gt; 2 StackedEnsemble_BestOfFamily_3_AutoML_1_20240624_180152 0.2232356 0.04983415\n#&gt; 3    StackedEnsemble_AllModels_2_AutoML_1_20240624_180152 0.2234426 0.04992660\n#&gt; 4 StackedEnsemble_BestOfFamily_2_AutoML_1_20240624_180152 0.2235772 0.04998677\n#&gt; 5    StackedEnsemble_AllModels_1_AutoML_1_20240624_180152 0.2239295 0.05014444\n#&gt; 6                          GBM_4_AutoML_1_20240624_180152 0.2269256 0.05149525\n#&gt;         mae     rmsle mean_residual_deviance\n#&gt; 1 0.1137959 0.1558245             0.04961620\n#&gt; 2 0.1132481 0.1559764             0.04983415\n#&gt; 3 0.1141648 0.1563323             0.04992660\n#&gt; 4 0.1134652 0.1558863             0.04998677\n#&gt; 5 0.1148692 0.1564373             0.05014444\n#&gt; 6 0.1161654 0.1577393             0.05149525\n#&gt; \n#&gt; [54 rows x 6 columns]\n\nextract_h2o_model_name_by_position &lt;- function(h2o_leaderboard, n = 1, verbose = T) {\n  \n  model_name &lt;- h2o_leaderboard %&gt;%\n    as.tibble() %&gt;%\n    slice_(n) %&gt;%\n    pull(model_id)\n  \n  if (verbose) message(model_name)\n  \n  return(model_name)\n  \n}\n\n## Predicting using Leader Model\nbest_model &lt;- automl_models_h2o@leaderboard %&gt;% \n  extract_h2o_model_name_by_position(1) %&gt;% \n  h2o.getModel()\n\n#&gt; Warning: `slice_()` was deprecated in dplyr 0.7.0.\n#&gt; ℹ Please use `slice()` instead.\n\n\n#&gt; Warning: `as.tibble()` was deprecated in tibble 2.0.0.\n#&gt; ℹ Please use `as_tibble()` instead.\n#&gt; ℹ The signature and semantics have changed, see `?as_tibble`.\n\n\n#&gt; StackedEnsemble_AllModels_3_AutoML_1_20240624_180152\n\npredictions &lt;- h2o.predict(best_model, newdata = as.h2o(test_tbl))\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntypeof(predictions)\n\n#&gt; [1] \"environment\"\n\npredictions_tbl &lt;- predictions %&gt;% as_tibble()\nglimpse(predictions_tbl)\n\n#&gt; Rows: 4,764\n#&gt; Columns: 1\n#&gt; $ predict &lt;dbl&gt; 0.6501088, 0.8170880, 0.3055763, 0.2352949, 0.8169238, 0.16940…\n\n## Save the leader model\nbest_model %&gt;% h2o.saveModel(path = \"StackedEnsemble_AllModels_AutoML_20220603_533865\")\n\n#&gt; [1] \"C:\\\\Users\\\\risho\\\\OneDrive\\\\Desktop\\\\internship_sparks\\\\ss24-bdml-rishon1234\\\\content\\\\01_journal\\\\StackedEnsemble_AllModels_AutoML_20220603_533865\\\\StackedEnsemble_AllModels_3_AutoML_1_20240624_180152\""
  },
  {
    "objectID": "content/01_journal/06_deep_learning.html",
    "href": "content/01_journal/06_deep_learning.html",
    "title": "06 Deep Learning",
    "section": "",
    "text": "IMPORTANT: You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.\nThis is an .Rmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header."
  },
  {
    "objectID": "content/01_journal/06_deep_learning.html#second-level-header",
    "href": "content/01_journal/06_deep_learning.html#second-level-header",
    "title": "06 Deep Learning",
    "section": "1.1 Second level header",
    "text": "1.1 Second level header\nYou can add more headers by adding more hashtags. These won’t be put into the table of contents\n\nthird level header\nHere’s an even lower level header"
  },
  {
    "objectID": "content/01_journal/04_performance_measures.html#gain-chart",
    "href": "content/01_journal/04_performance_measures.html#gain-chart",
    "title": "04 Performance Measures",
    "section": "12.1 Gain Chart",
    "text": "12.1 Gain Chart\ngain_transformed_tbl &lt;- gain_lift_tbl %&gt;% select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %&gt;% select(-contains(“lift”)) %&gt;% mutate(baseline = cumulative_data_fraction) %&gt;% rename(gain = cumulative_capture_rate) %&gt;% # prepare the data for the plotting (for the color and group aesthetics) pivot_longer(cols = c(gain, baseline), values_to = “value”, names_to = “key”)\ngain_transformed_tbl %&gt;% ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) + geom_line(size = 1.5) + labs( title = “Gain Chart”, x = “Cumulative Data Fraction”, y = “Gain” ) + theme_new\n\n\n\nGain Plot"
  },
  {
    "objectID": "content/01_journal/05_lime.html",
    "href": "content/01_journal/05_lime.html",
    "title": "05 LIME",
    "section": "",
    "text": "library(h2o)\n\n#&gt; \n#&gt; ----------------------------------------------------------------------\n#&gt; \n#&gt; Your next step is to start H2O:\n#&gt;     &gt; h2o.init()\n#&gt; \n#&gt; For H2O package documentation, ask for help:\n#&gt;     &gt; ??h2o\n#&gt; \n#&gt; After starting H2O, you can use the Web UI at http://localhost:54321\n#&gt; For more information visit https://docs.h2o.ai\n#&gt; \n#&gt; ----------------------------------------------------------------------\n\n\n#&gt; \n#&gt; Attaching package: 'h2o'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     cor, sd, var\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames,\n#&gt;     colnames&lt;-, ifelse, is.character, is.factor, is.numeric, log,\n#&gt;     log10, log1p, log2, round, signif, trunc\n\nlibrary(recipes)\n\n#&gt; Loading required package: dplyr\n\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\n\n#&gt; \n#&gt; Attaching package: 'recipes'\n\n\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     step\n\nlibrary(readxl)\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ forcats   1.0.0     ✔ readr     2.1.5\n#&gt; ✔ ggplot2   3.5.1     ✔ stringr   1.5.1\n#&gt; ✔ lubridate 1.9.3     ✔ tibble    3.2.1\n#&gt; ✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ lubridate::day()   masks h2o::day()\n#&gt; ✖ dplyr::filter()    masks stats::filter()\n#&gt; ✖ stringr::fixed()   masks recipes::fixed()\n#&gt; ✖ lubridate::hour()  masks h2o::hour()\n#&gt; ✖ dplyr::lag()       masks stats::lag()\n#&gt; ✖ lubridate::month() masks h2o::month()\n#&gt; ✖ lubridate::week()  masks h2o::week()\n#&gt; ✖ lubridate::year()  masks h2o::year()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidyquant)\n\n#&gt; Loading required package: PerformanceAnalytics\n#&gt; Loading required package: xts\n#&gt; Loading required package: zoo\n#&gt; \n#&gt; Attaching package: 'zoo'\n#&gt; \n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     as.Date, as.Date.numeric\n#&gt; \n#&gt; \n#&gt; ######################### Warning from 'xts' package ##########################\n#&gt; #                                                                             #\n#&gt; # The dplyr lag() function breaks how base R's lag() function is supposed to  #\n#&gt; # work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n#&gt; # source() into this session won't work correctly.                            #\n#&gt; #                                                                             #\n#&gt; # Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n#&gt; # conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n#&gt; # dplyr from breaking base R's lag() function.                                #\n#&gt; #                                                                             #\n#&gt; # Code in packages is not affected. It's protected by R's namespace mechanism #\n#&gt; # Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#&gt; #                                                                             #\n#&gt; ###############################################################################\n#&gt; \n#&gt; Attaching package: 'xts'\n#&gt; \n#&gt; The following objects are masked from 'package:dplyr':\n#&gt; \n#&gt;     first, last\n#&gt; \n#&gt; \n#&gt; Attaching package: 'PerformanceAnalytics'\n#&gt; \n#&gt; The following object is masked from 'package:graphics':\n#&gt; \n#&gt;     legend\n#&gt; \n#&gt; Loading required package: quantmod\n#&gt; Loading required package: TTR\n#&gt; Registered S3 method overwritten by 'quantmod':\n#&gt;   method            from\n#&gt;   as.zoo.data.frame zoo\n\nlibrary(lime)\n\n#&gt; \n#&gt; Attaching package: 'lime'\n#&gt; \n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     explain\n\nlibrary(rsample)\n\nproduct_data &lt;- read_csv(\"C:/Users/risho/OneDrive/Desktop/internship_sparks/ss24-bdml-rishon1234/ML/data/Business Decisions with Machine Learning/product_backorders.csv\")\n\n#&gt; Rows: 19053 Columns: 23\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr  (7): potential_issue, deck_risk, oe_constraint, ppap_risk, stop_auto_bu...\n#&gt; dbl (16): sku, national_inv, lead_time, in_transit_qty, forecast_3_month, fo...\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nproduct_data2 &lt;- product_data %&gt;% \n  mutate(\n    product_backorder = went_on_backorder %&gt;% str_to_lower() %&gt;% str_detect(\"yes\") %&gt;% as.numeric()\n  ) %&gt;% \n  select(-c(went_on_backorder))\nglimpse(product_data)\n\n#&gt; Rows: 19,053\n#&gt; Columns: 23\n#&gt; $ sku               &lt;dbl&gt; 1113121, 1113268, 1113874, 1114222, 1114823, 1115453…\n#&gt; $ national_inv      &lt;dbl&gt; 0, 0, 20, 0, 0, 55, -34, 4, 2, -7, 1, 2, 0, 0, 0, 0,…\n#&gt; $ lead_time         &lt;dbl&gt; 8, 8, 2, 8, 12, 8, 8, 9, 8, 8, 8, 8, 12, 2, 12, 4, 2…\n#&gt; $ in_transit_qty    &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…\n#&gt; $ forecast_3_month  &lt;dbl&gt; 6, 2, 45, 9, 31, 216, 120, 43, 4, 56, 2, 5, 5, 54, 4…\n#&gt; $ forecast_6_month  &lt;dbl&gt; 6, 3, 99, 14, 31, 360, 240, 67, 6, 96, 4, 9, 6, 72, …\n#&gt; $ forecast_9_month  &lt;dbl&gt; 6, 4, 153, 21, 31, 492, 240, 115, 9, 112, 6, 13, 9, …\n#&gt; $ sales_1_month     &lt;dbl&gt; 0, 1, 16, 5, 7, 30, 83, 5, 1, 13, 0, 1, 0, 0, 1, 0, …\n#&gt; $ sales_3_month     &lt;dbl&gt; 4, 2, 42, 17, 15, 108, 122, 22, 5, 30, 2, 5, 4, 0, 3…\n#&gt; $ sales_6_month     &lt;dbl&gt; 9, 3, 80, 36, 33, 275, 144, 40, 6, 56, 3, 8, 5, 0, 4…\n#&gt; $ sales_9_month     &lt;dbl&gt; 12, 3, 111, 43, 47, 340, 165, 58, 9, 76, 4, 11, 6, 0…\n#&gt; $ min_bank          &lt;dbl&gt; 0, 0, 10, 0, 2, 51, 33, 4, 2, 0, 0, 0, 3, 4, 0, 0, 0…\n#&gt; $ potential_issue   &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n#&gt; $ pieces_past_due   &lt;dbl&gt; 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#&gt; $ perf_6_month_avg  &lt;dbl&gt; 0.90, 0.96, 0.81, 0.96, 0.98, 0.00, 1.00, 0.69, 1.00…\n#&gt; $ perf_12_month_avg &lt;dbl&gt; 0.89, 0.97, 0.88, 0.98, 0.98, 0.00, 0.97, 0.68, 0.95…\n#&gt; $ local_bo_qty      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 34, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ deck_risk         &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n#&gt; $ oe_constraint     &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n#&gt; $ ppap_risk         &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No…\n#&gt; $ stop_auto_buy     &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Ye…\n#&gt; $ rev_stop          &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n#&gt; $ went_on_backorder &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Ye…\n\nsplit_obj&lt;- initial_split(product_data2, prop = 0.75)\ntrain_tbl&lt;- training(split_obj)\ntest_tbl&lt;- testing(split_obj)\n\nrecipe_obj &lt;- recipe(product_backorder ~., data = train_tbl) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_dummy(all_nominal(),-all_outcomes()) %&gt;%\n  prep()\nsplit_obj&lt;- initial_split(product_data2, prop = 0.75)\ntrain_tbl&lt;- training(split_obj)\ntest_tbl&lt;- testing(split_obj)\n\nh2o.init()\n\n#&gt;  Connection successful!\n#&gt; \n#&gt; R is connected to the H2O cluster: \n#&gt;     H2O cluster uptime:         46 minutes 29 seconds \n#&gt;     H2O cluster timezone:       Europe/Berlin \n#&gt;     H2O data parsing timezone:  UTC \n#&gt;     H2O cluster version:        3.44.0.3 \n#&gt;     H2O cluster version age:    6 months and 3 days \n#&gt;     H2O cluster name:           H2O_started_from_R_risho_zzl870 \n#&gt;     H2O cluster total nodes:    1 \n#&gt;     H2O cluster total memory:   1.63 GB \n#&gt;     H2O cluster total cores:    12 \n#&gt;     H2O cluster allowed cores:  12 \n#&gt;     H2O cluster healthy:        TRUE \n#&gt;     H2O Connection ip:          localhost \n#&gt;     H2O Connection port:        54321 \n#&gt;     H2O Connection proxy:       NA \n#&gt;     H2O Internal Security:      FALSE \n#&gt;     R Version:                  R version 4.4.0 (2024-04-24 ucrt)\n\n\n#&gt; Warning in h2o.clusterInfo(): \n#&gt; Your H2O cluster version is (6 months and 3 days) old. There may be a newer version available.\n#&gt; Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n\nautoml_leader &lt;- h2o.loadModel(\"C:/Users/risho/OneDrive/Desktop/internship_sparks/ss24-bdml-rishon1234/ML/StackedEnsemble_AllModels_AutoML_20220603_533865/StackedEnsemble_AllModels_3_AutoML_1_20240622_142532\")\nautoml_leader\n\n#&gt; Model Details:\n#&gt; ==============\n#&gt; \n#&gt; H2ORegressionModel: stackedensemble\n#&gt; Model ID:  StackedEnsemble_AllModels_3_AutoML_1_20240622_142532 \n#&gt; Model Summary for Stacked Ensemble: \n#&gt;                                          key            value\n#&gt; 1                          Stacking strategy cross_validation\n#&gt; 2       Number of base models (used / total)            19/50\n#&gt; 3           # GBM base models (used / total)            17/42\n#&gt; 4           # DRF base models (used / total)              2/2\n#&gt; 5  # DeepLearning base models (used / total)              0/5\n#&gt; 6           # GLM base models (used / total)              0/1\n#&gt; 7                      Metalearner algorithm              GLM\n#&gt; 8         Metalearner fold assignment scheme           Random\n#&gt; 9                         Metalearner nfolds                5\n#&gt; 10                   Metalearner fold_column               NA\n#&gt; 11        Custom metalearner hyperparameters             None\n#&gt; \n#&gt; \n#&gt; H2ORegressionMetrics: stackedensemble\n#&gt; ** Reported on training data. **\n#&gt; \n#&gt; MSE:  0.01822598\n#&gt; RMSE:  0.1350036\n#&gt; MAE:  0.06575185\n#&gt; RMSLE:  0.09474208\n#&gt; Mean Residual Deviance :  0.01822598\n#&gt; \n#&gt; \n#&gt; H2ORegressionMetrics: stackedensemble\n#&gt; ** Reported on validation data. **\n#&gt; \n#&gt; MSE:  0.05120148\n#&gt; RMSE:  0.2262774\n#&gt; MAE:  0.1102032\n#&gt; RMSLE:  0.1590233\n#&gt; Mean Residual Deviance :  0.05120148\n#&gt; \n#&gt; \n#&gt; H2ORegressionMetrics: stackedensemble\n#&gt; ** Reported on cross-validation data. **\n#&gt; ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#&gt; \n#&gt; MSE:  0.05040252\n#&gt; RMSE:  0.2245051\n#&gt; MAE:  0.1120714\n#&gt; RMSLE:  0.1578221\n#&gt; Mean Residual Deviance :  0.05040252\n#&gt; \n#&gt; \n#&gt; Cross-Validation Metrics Summary: \n#&gt;                              mean        sd cv_1_valid cv_2_valid cv_3_valid\n#&gt; mae                      0.112069  0.003868   0.110514   0.118646   0.111860\n#&gt; mean_residual_deviance   0.050346  0.003496   0.050188   0.055389   0.051883\n#&gt; mse                      0.050346  0.003496   0.050188   0.055389   0.051883\n#&gt; null_deviance          224.017940 12.613392 220.105120 244.018360 227.994700\n#&gt; r2                       0.517080  0.016213   0.498813   0.511007   0.507770\n#&gt; residual_deviance      108.197270  8.415395 110.263910 119.086914 112.222010\n#&gt; rmse                     0.224273  0.007750   0.224028   0.235349   0.227777\n#&gt; rmsle                    0.157645  0.004187   0.157953   0.164196   0.157973\n#&gt;                        cv_4_valid cv_5_valid\n#&gt; mae                      0.108535   0.110791\n#&gt; mean_residual_deviance   0.046653   0.047618\n#&gt; mse                      0.046653   0.047618\n#&gt; null_deviance          215.392840 212.578670\n#&gt; r2                       0.537650   0.530161\n#&gt; residual_deviance       99.557594  99.855950\n#&gt; rmse                     0.215993   0.218217\n#&gt; rmsle                    0.153275   0.154830\n\npredictions_tbl &lt;- automl_leader %&gt;% \n  h2o.predict(newdata = as.h2o(test_tbl)) %&gt;%\n  as.tibble() %&gt;%\n  bind_cols(\n    test_tbl %&gt;%\n      select(everything())\n  )\n\n#&gt; Warning: `as.tibble()` was deprecated in tibble 2.0.0.\n#&gt; ℹ Please use `as_tibble()` instead.\n#&gt; ℹ The signature and semantics have changed, see `?as_tibble`.\n\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\npredictions_tbl\n\n\n  \n\n\nsummary(train_tbl)\n\n#&gt;       sku           national_inv        lead_time      in_transit_qty   \n#&gt;  Min.   :1112168   Min.   : -1440.0   Min.   : 0.000   Min.   :    0.0  \n#&gt;  1st Qu.:1512506   1st Qu.:     3.0   1st Qu.: 4.000   1st Qu.:    0.0  \n#&gt;  Median :1922957   Median :    12.0   Median : 8.000   Median :    0.0  \n#&gt;  Mean   :2061243   Mean   :   296.7   Mean   : 7.762   Mean   :   41.6  \n#&gt;  3rd Qu.:2831388   3rd Qu.:    65.0   3rd Qu.: 8.000   3rd Qu.:    0.0  \n#&gt;  Max.   :3284775   Max.   :346702.0   Max.   :52.000   Max.   :68955.0  \n#&gt;                                       NA's   :815                       \n#&gt;  forecast_3_month   forecast_6_month   forecast_9_month  sales_1_month      \n#&gt;  Min.   :     0.0   Min.   :     0.0   Min.   :      0   Min.   :     0.00  \n#&gt;  1st Qu.:     0.0   1st Qu.:     0.0   1st Qu.:      0   1st Qu.:     0.00  \n#&gt;  Median :     0.0   Median :     0.0   Median :      0   Median :     0.00  \n#&gt;  Mean   :   183.8   Mean   :   341.7   Mean   :    492   Mean   :    54.98  \n#&gt;  3rd Qu.:     9.0   3rd Qu.:    20.0   3rd Qu.:     30   3rd Qu.:     5.00  \n#&gt;  Max.   :479808.0   Max.   :967776.0   Max.   :1418208   Max.   :186451.00  \n#&gt;                                                                             \n#&gt;  sales_3_month      sales_6_month       sales_9_month          min_bank       \n#&gt;  Min.   :     0.0   Min.   :      0.0   Min.   :      0.0   Min.   :    0.00  \n#&gt;  1st Qu.:     0.0   1st Qu.:      0.0   1st Qu.:      0.0   1st Qu.:    0.00  \n#&gt;  Median :     1.0   Median :      3.0   Median :      5.0   Median :    0.00  \n#&gt;  Mean   :   167.9   Mean   :    330.9   Mean   :    500.8   Mean   :   48.85  \n#&gt;  3rd Qu.:    16.0   3rd Qu.:     32.0   3rd Qu.:     47.0   3rd Qu.:    3.00  \n#&gt;  Max.   :550609.0   Max.   :1136154.0   Max.   :1759152.0   Max.   :85584.00  \n#&gt;                                                                               \n#&gt;  potential_issue    pieces_past_due     perf_6_month_avg  perf_12_month_avg\n#&gt;  Length:14289       Min.   :    0.000   Min.   :-99.000   Min.   :-99.000  \n#&gt;  Class :character   1st Qu.:    0.000   1st Qu.:  0.630   1st Qu.:  0.660  \n#&gt;  Mode  :character   Median :    0.000   Median :  0.820   Median :  0.800  \n#&gt;                     Mean   :    2.505   Mean   : -6.584   Mean   : -6.108  \n#&gt;                     3rd Qu.:    0.000   3rd Qu.:  0.960   3rd Qu.:  0.950  \n#&gt;                     Max.   :13824.000   Max.   :  1.000   Max.   :  1.000  \n#&gt;                                                                            \n#&gt;   local_bo_qty        deck_risk         oe_constraint       ppap_risk        \n#&gt;  Min.   :   0.0000   Length:14289       Length:14289       Length:14289      \n#&gt;  1st Qu.:   0.0000   Class :character   Class :character   Class :character  \n#&gt;  Median :   0.0000   Mode  :character   Mode  :character   Mode  :character  \n#&gt;  Mean   :   0.8957                                                           \n#&gt;  3rd Qu.:   0.0000                                                           \n#&gt;  Max.   :1440.0000                                                           \n#&gt;                                                                              \n#&gt;  stop_auto_buy        rev_stop         product_backorder\n#&gt;  Length:14289       Length:14289       Min.   :0.0000   \n#&gt;  Class :character   Class :character   1st Qu.:0.0000   \n#&gt;  Mode  :character   Mode  :character   Median :0.0000   \n#&gt;                                        Mean   :0.1192   \n#&gt;                                        3rd Qu.:0.0000   \n#&gt;                                        Max.   :1.0000   \n#&gt; \n\n## Original plot_features()\n\n# explanation %&gt;% \n#   as.tibble()\n#   \n# case_1 &lt;- explanation %&gt;%\n#     filter(case == 1)\n# \n# case_1 %&gt;%\n#     plot_features()\n# You will need at least the layers geom_col() and coord_flip().\nexplainer &lt;- train_tbl %&gt;%\n  select(-product_backorder) %&gt;%\n  lime(\n    model           = automl_leader,\n    bin_continuous  = TRUE,\n    n_bins          = 4,\n    quantile_bins   = TRUE\n  )\n\n#&gt; Warning: in_transit_qty does not contain enough variance to use quantile\n#&gt; binning. Using standard binning instead.\n\n\n#&gt; Warning: pieces_past_due does not contain enough variance to use quantile\n#&gt; binning. Using standard binning instead.\n\n\n#&gt; Warning: local_bo_qty does not contain enough variance to use quantile binning.\n#&gt; Using standard binning instead.\n\nexplainer\n\n#&gt; $model\n#&gt; Model Details:\n#&gt; ==============\n#&gt; \n#&gt; H2ORegressionModel: stackedensemble\n#&gt; Model ID:  StackedEnsemble_AllModels_3_AutoML_1_20240622_142532 \n#&gt; Model Summary for Stacked Ensemble: \n#&gt;                                          key            value\n#&gt; 1                          Stacking strategy cross_validation\n#&gt; 2       Number of base models (used / total)            19/50\n#&gt; 3           # GBM base models (used / total)            17/42\n#&gt; 4           # DRF base models (used / total)              2/2\n#&gt; 5  # DeepLearning base models (used / total)              0/5\n#&gt; 6           # GLM base models (used / total)              0/1\n#&gt; 7                      Metalearner algorithm              GLM\n#&gt; 8         Metalearner fold assignment scheme           Random\n#&gt; 9                         Metalearner nfolds                5\n#&gt; 10                   Metalearner fold_column               NA\n#&gt; 11        Custom metalearner hyperparameters             None\n#&gt; \n#&gt; \n#&gt; H2ORegressionMetrics: stackedensemble\n#&gt; ** Reported on training data. **\n#&gt; \n#&gt; MSE:  0.01822598\n#&gt; RMSE:  0.1350036\n#&gt; MAE:  0.06575185\n#&gt; RMSLE:  0.09474208\n#&gt; Mean Residual Deviance :  0.01822598\n#&gt; \n#&gt; \n#&gt; H2ORegressionMetrics: stackedensemble\n#&gt; ** Reported on validation data. **\n#&gt; \n#&gt; MSE:  0.05120148\n#&gt; RMSE:  0.2262774\n#&gt; MAE:  0.1102032\n#&gt; RMSLE:  0.1590233\n#&gt; Mean Residual Deviance :  0.05120148\n#&gt; \n#&gt; \n#&gt; H2ORegressionMetrics: stackedensemble\n#&gt; ** Reported on cross-validation data. **\n#&gt; ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#&gt; \n#&gt; MSE:  0.05040252\n#&gt; RMSE:  0.2245051\n#&gt; MAE:  0.1120714\n#&gt; RMSLE:  0.1578221\n#&gt; Mean Residual Deviance :  0.05040252\n#&gt; \n#&gt; \n#&gt; Cross-Validation Metrics Summary: \n#&gt;                              mean        sd cv_1_valid cv_2_valid cv_3_valid\n#&gt; mae                      0.112069  0.003868   0.110514   0.118646   0.111860\n#&gt; mean_residual_deviance   0.050346  0.003496   0.050188   0.055389   0.051883\n#&gt; mse                      0.050346  0.003496   0.050188   0.055389   0.051883\n#&gt; null_deviance          224.017940 12.613392 220.105120 244.018360 227.994700\n#&gt; r2                       0.517080  0.016213   0.498813   0.511007   0.507770\n#&gt; residual_deviance      108.197270  8.415395 110.263910 119.086914 112.222010\n#&gt; rmse                     0.224273  0.007750   0.224028   0.235349   0.227777\n#&gt; rmsle                    0.157645  0.004187   0.157953   0.164196   0.157973\n#&gt;                        cv_4_valid cv_5_valid\n#&gt; mae                      0.108535   0.110791\n#&gt; mean_residual_deviance   0.046653   0.047618\n#&gt; mse                      0.046653   0.047618\n#&gt; null_deviance          215.392840 212.578670\n#&gt; r2                       0.537650   0.530161\n#&gt; residual_deviance       99.557594  99.855950\n#&gt; rmse                     0.215993   0.218217\n#&gt; rmsle                    0.153275   0.154830\n#&gt; \n#&gt; $preprocess\n#&gt; function (x) \n#&gt; x\n#&gt; &lt;bytecode: 0x000001c4c58366a0&gt;\n#&gt; &lt;environment: 0x000001c4c582ec18&gt;\n#&gt; \n#&gt; $bin_continuous\n#&gt; [1] TRUE\n#&gt; \n#&gt; $n_bins\n#&gt; [1] 4\n#&gt; \n#&gt; $quantile_bins\n#&gt; [1] TRUE\n#&gt; \n#&gt; $use_density\n#&gt; [1] TRUE\n#&gt; \n#&gt; $feature_type\n#&gt;               sku      national_inv         lead_time    in_transit_qty \n#&gt;         \"numeric\"         \"numeric\"         \"numeric\"         \"numeric\" \n#&gt;  forecast_3_month  forecast_6_month  forecast_9_month     sales_1_month \n#&gt;         \"numeric\"         \"numeric\"         \"numeric\"         \"numeric\" \n#&gt;     sales_3_month     sales_6_month     sales_9_month          min_bank \n#&gt;         \"numeric\"         \"numeric\"         \"numeric\"         \"numeric\" \n#&gt;   potential_issue   pieces_past_due  perf_6_month_avg perf_12_month_avg \n#&gt;       \"character\"         \"numeric\"         \"numeric\"         \"numeric\" \n#&gt;      local_bo_qty         deck_risk     oe_constraint         ppap_risk \n#&gt;         \"numeric\"       \"character\"       \"character\"       \"character\" \n#&gt;     stop_auto_buy          rev_stop \n#&gt;       \"character\"       \"character\" \n#&gt; \n#&gt; $bin_cuts\n#&gt; $bin_cuts$sku\n#&gt;      0%     25%     50%     75%    100% \n#&gt; 1112168 1512506 1922957 2831388 3284775 \n#&gt; \n#&gt; $bin_cuts$national_inv\n#&gt;     0%    25%    50%    75%   100% \n#&gt;  -1440      3     12     65 346702 \n#&gt; \n#&gt; $bin_cuts$lead_time\n#&gt;   0%  25%  50% 100% \n#&gt;    0    4    8   52 \n#&gt; \n#&gt; $bin_cuts$in_transit_qty\n#&gt; [1]     0.00 17238.75 34477.50 51716.25 68955.00\n#&gt; \n#&gt; $bin_cuts$forecast_3_month\n#&gt;     0%    75%   100% \n#&gt;      0      9 479808 \n#&gt; \n#&gt; $bin_cuts$forecast_6_month\n#&gt;     0%    75%   100% \n#&gt;      0     20 967776 \n#&gt; \n#&gt; $bin_cuts$forecast_9_month\n#&gt;      0%     75%    100% \n#&gt;       0      30 1418208 \n#&gt; \n#&gt; $bin_cuts$sales_1_month\n#&gt;     0%    75%   100% \n#&gt;      0      5 186451 \n#&gt; \n#&gt; $bin_cuts$sales_3_month\n#&gt;     0%    50%    75%   100% \n#&gt;      0      1     16 550609 \n#&gt; \n#&gt; $bin_cuts$sales_6_month\n#&gt;      0%     50%     75%    100% \n#&gt;       0       3      32 1136154 \n#&gt; \n#&gt; $bin_cuts$sales_9_month\n#&gt;      0%     50%     75%    100% \n#&gt;       0       5      47 1759152 \n#&gt; \n#&gt; $bin_cuts$min_bank\n#&gt;    0%   75%  100% \n#&gt;     0     3 85584 \n#&gt; \n#&gt; $bin_cuts$potential_issue\n#&gt; NULL\n#&gt; \n#&gt; $bin_cuts$pieces_past_due\n#&gt; [1]     0  3456  6912 10368 13824\n#&gt; \n#&gt; $bin_cuts$perf_6_month_avg\n#&gt;     0%    25%    50%    75%   100% \n#&gt; -99.00   0.63   0.82   0.96   1.00 \n#&gt; \n#&gt; $bin_cuts$perf_12_month_avg\n#&gt;     0%    25%    50%    75%   100% \n#&gt; -99.00   0.66   0.80   0.95   1.00 \n#&gt; \n#&gt; $bin_cuts$local_bo_qty\n#&gt; [1]    0  360  720 1080 1440\n#&gt; \n#&gt; $bin_cuts$deck_risk\n#&gt; NULL\n#&gt; \n#&gt; $bin_cuts$oe_constraint\n#&gt; NULL\n#&gt; \n#&gt; $bin_cuts$ppap_risk\n#&gt; NULL\n#&gt; \n#&gt; $bin_cuts$stop_auto_buy\n#&gt; NULL\n#&gt; \n#&gt; $bin_cuts$rev_stop\n#&gt; NULL\n#&gt; \n#&gt; \n#&gt; $feature_distribution\n#&gt; $feature_distribution$sku\n#&gt; \n#&gt;         1         2         3         4 \n#&gt; 0.2500525 0.2499825 0.2499825 0.2499825 \n#&gt; \n#&gt; $feature_distribution$national_inv\n#&gt; \n#&gt;         1         2         3         4 \n#&gt; 0.2732172 0.2395549 0.2378053 0.2494226 \n#&gt; \n#&gt; $feature_distribution$lead_time\n#&gt; \n#&gt;         1         2         3 \n#&gt; 0.2972216 0.4132550 0.2324865 \n#&gt; \n#&gt; $feature_distribution$in_transit_qty\n#&gt; \n#&gt;            1            2            3            4 \n#&gt; 0.9996500805 0.0002099517 0.0000699839 0.0000699839 \n#&gt; \n#&gt; $feature_distribution$forecast_3_month\n#&gt; \n#&gt;         1         2 \n#&gt; 0.7519071 0.2480929 \n#&gt; \n#&gt; $feature_distribution$forecast_6_month\n#&gt; \n#&gt;         1         2 \n#&gt; 0.7533067 0.2466933 \n#&gt; \n#&gt; $feature_distribution$forecast_9_month\n#&gt; \n#&gt;         1         2 \n#&gt; 0.7532368 0.2467632 \n#&gt; \n#&gt; $feature_distribution$sales_1_month\n#&gt; \n#&gt;         1         2 \n#&gt; 0.7603751 0.2396249 \n#&gt; \n#&gt; $feature_distribution$sales_3_month\n#&gt; \n#&gt;         1         2         3 \n#&gt; 0.5002449 0.2513122 0.2484429 \n#&gt; \n#&gt; $feature_distribution$sales_6_month\n#&gt; \n#&gt;         1         2         3 \n#&gt; 0.5074533 0.2436140 0.2489327 \n#&gt; \n#&gt; $feature_distribution$sales_9_month\n#&gt; \n#&gt;         1         2         3 \n#&gt; 0.5123522 0.2382952 0.2493526 \n#&gt; \n#&gt; $feature_distribution$min_bank\n#&gt; \n#&gt;         1         2 \n#&gt; 0.7541465 0.2458535 \n#&gt; \n#&gt; $feature_distribution$potential_issue\n#&gt; \n#&gt;         No        Yes \n#&gt; 0.99874029 0.00125971 \n#&gt; \n#&gt; $feature_distribution$pieces_past_due\n#&gt; \n#&gt;           1           2           4 \n#&gt; 9.99860e-01 6.99839e-05 6.99839e-05 \n#&gt; \n#&gt; $feature_distribution$perf_6_month_avg\n#&gt; \n#&gt;         1         2         3         4 \n#&gt; 0.2625096 0.2556512 0.2334663 0.2483729 \n#&gt; \n#&gt; $feature_distribution$perf_12_month_avg\n#&gt; \n#&gt;         1         2         3         4 \n#&gt; 0.2808454 0.2238085 0.2648191 0.2305270 \n#&gt; \n#&gt; $feature_distribution$local_bo_qty\n#&gt; \n#&gt;            1            2            3            4 \n#&gt; 0.9993701449 0.0003499195 0.0000699839 0.0002099517 \n#&gt; \n#&gt; $feature_distribution$deck_risk\n#&gt; \n#&gt;        No       Yes \n#&gt; 0.7793408 0.2206592 \n#&gt; \n#&gt; $feature_distribution$oe_constraint\n#&gt; \n#&gt;           No          Yes \n#&gt; 0.9997900483 0.0002099517 \n#&gt; \n#&gt; $feature_distribution$ppap_risk\n#&gt; \n#&gt;        No       Yes \n#&gt; 0.8770383 0.1229617 \n#&gt; \n#&gt; $feature_distribution$stop_auto_buy\n#&gt; \n#&gt;         No        Yes \n#&gt; 0.03667157 0.96332843 \n#&gt; \n#&gt; $feature_distribution$rev_stop\n#&gt; \n#&gt;           No          Yes \n#&gt; 0.9994401288 0.0005598712 \n#&gt; \n#&gt; \n#&gt; attr(,\"class\")\n#&gt; [1] \"data_frame_explainer\" \"explainer\"            \"list\"\n\nexplanation &lt;- test_tbl %&gt;%\n  slice(1) %&gt;%\n  select(-product_backorder) %&gt;%\n  lime::explain(\n    \n    # Pass our explainer object\n    explainer = explainer,\n    # Because it is a binary classification model: 1\n    n_labels   = 1,\n    # number of features to be returned\n    n_features = 8,\n    # number of localized linear models\n    n_permutations = 5000,\n    # Let's start with 1\n    kernel_width   = 1\n  )\n\n#&gt; Warning in explain.data.frame(., explainer = explainer, n_labels = 1,\n#&gt; n_features = 8, : \"labels\" and \"n_labels\" arguments are ignored when explaining\n#&gt; regression models\n\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nexplanation\n\n\n  \n\n\ng &lt;- plot_features(explanation = explanation, ncol = 1, cases = 1)\ng\n\n\n\n\n\n\n\n## Bonus Objectives:\n\nexplanation_multi &lt;- test_tbl %&gt;%\n  slice(1:20) %&gt;%\n  select(-product_backorder) %&gt;%\n  lime::explain(\n    explainer = explainer,\n    n_labels   = 1,\n    n_features = 8,\n    n_permutations = 5000,\n    kernel_width   = 0.5\n  )\n\n#&gt; Warning in explain.data.frame(., explainer = explainer, n_labels = 1,\n#&gt; n_features = 8, : \"labels\" and \"n_labels\" arguments are ignored when explaining\n#&gt; regression models\n\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nexplanation_multi %&gt;%\n  as.tibble()\n\n\n  \n\n\nplot_explanations(explanation_multi)\n\n#&gt; Warning: Unknown or uninitialised column: `label`.\n\n\n\n\n\n\n\n\n# Part 2: Recreate plot_explanations():\n## Customized plot_features()\n\ntheme_lime &lt;- function(...) {\n  theme_minimal() +\n    theme(\n      strip.text = element_text(face = 'bold', size = 9),\n      plot.margin = margin(15, 15, 15, 15),\n      legend.background = element_blank(),\n      legend.key = element_blank(),\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor.y = element_blank(),\n      axis.ticks = element_blank(),\n      legend.position = 'bottom',\n      panel.spacing.y = unit(15, 'pt'),\n      strip.text.x = element_text(margin = margin(t = 2, b = 2), hjust = 0),\n      axis.title.y = element_text(margin = margin(r = 10)),\n      axis.title.x = element_text(margin = margin(t = 10)),\n      panel.background = element_rect(fill   = \"transparent\"),\n      panel.border     = element_rect(color = \"black\", fill = NA, size = 0.5),\n      panel.grid.major = element_line(color = \"grey\", size = 0.333),\n      ...\n    )\n}\n\nplot_explanations_customized &lt;- function(explanation, ...) {\n  num_cases &lt;- unique(suppressWarnings(as.numeric(explanation$case)))\n  if (!anyNA(num_cases)) {\n    explanation$case &lt;- factor(explanation$case, levels = as.character(sort(num_cases)))\n  }\n  explanation$feature_desc &lt;- factor(\n    explanation$feature_desc,\n    levels = rev(unique(explanation$feature_desc[order(explanation$feature, explanation$feature_value)]))\n  )\n  \n  \n  p &lt;- ggplot(explanation, aes_(~case, ~feature_desc),show.legend=TRUE) +\n    geom_tile(aes_(fill = ~feature_weight)) +\n    scale_x_discrete('Case', expand = c(0, 0)) +\n    scale_y_discrete('Feature', expand = c(0, 0)) +\n    scale_fill_gradient2('Feature\\nweight', low = 'firebrick', mid = '#f7f7f7', high = 'steelblue') +\n    theme_lime() +\n    theme(panel.border = element_rect(fill = NA, colour = 'grey60', size = 1),\n          panel.grid = element_blank(),\n          legend.position = 'right',\n          axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n  if (is.null(explanation$label)) {\n    p\n  } else {\n    p + facet_wrap(~label, ...)\n  }\n}\n\n\n\nplot_explanations_customized(explanation = explanation, ncol = 1, cases = 1)\n\n#&gt; Warning: `aes_()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`\n\n\n#&gt; Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\n#&gt; ℹ Please use the `linewidth` argument instead.\n\n\n#&gt; Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\n#&gt; ℹ Please use the `linewidth` argument instead.\n\n\n#&gt; Warning: Unknown or uninitialised column: `label`.\n\n\n\n\n\n\n\n\nplot_explanations_customized(explanation_multi)\n\n#&gt; Warning: Unknown or uninitialised column: `label`."
  },
  {
    "objectID": "content/01_journal/04_performance_measures.html",
    "href": "content/01_journal/04_performance_measures.html",
    "title": "04 Performance Measures",
    "section": "",
    "text": "As issues with running h2o in the qmd-file where detected, the complete code is also available in an R-file: 05_performance_measures.R. This file executes the complete code properly. Code parts in this qmd-file that couldn’t be run due to this issue with h2o where not executed, but only displayed. Instead the plots that I got from running the R file, are displayed."
  }
]